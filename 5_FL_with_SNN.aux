\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Enhancing Cooperative Multi-agent Reinforcement Learning through the Integration of R-STDP and Federated Learning}{53}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Consensus Flying Problem}{53}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The central server (the leader) and the surrounding follower agents (white drones). The follower agents learn to fly in a formation to maintain the commanded distance. The Local models trained individually by follower agents are sent to the leader. The leader aggregates the models and sends back the global model for another round of training of the follower agents.}}{54}{}\protected@file@percent }
\newlabel{fig.Consensus Flying}{{5.1}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Proposed Method}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Network Structure}{55}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \gls {snn} structure with encoding and decoding layers. Each sub-layer consists of a fuzzy encoder and a Fuzzy-to-Spiking Converter, with the output layer receiving inputs from synaptic weights and a random action selector. During the training phase, the output layer receives input only from the random action selector, which then shifts to synaptic weight inputs after the training.}}{56}{}\protected@file@percent }
\newlabel{fig.Network Structure}{{5.2}{56}}
\citation{NS}
\newlabel{Eq.NS.1}{{5.1}{57}}
\newlabel{Eq.NS.2}{{5.2}{57}}
\newlabel{Eq.NS.3}{{5.3}{57}}
\newlabel{Eq.NS.4}{{5.4}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The fuzzy encoding principle for the input sub-layer.}}{58}{}\protected@file@percent }
\newlabel{fig.Encoder}{{5.3}{58}}
\newlabel{Eq.NS.5}{{5.5}{58}}
\newlabel{Eq.NS.6}{{5.6}{58}}
\citation{STDP2}
\newlabel{Eq.NS.7}{{5.7}{59}}
\newlabel{Eq.NS.7.1}{{5.8}{59}}
\newlabel{Eq.NS.7.2}{{5.9}{59}}
\newlabel{Eq.NS.8}{{5.10}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Training algorithm}{59}{}\protected@file@percent }
\citation{STDP1}
\newlabel{Eq.TA.1}{{5.11}{60}}
\newlabel{Eq.TA.2}{{5.12}{60}}
\newlabel{Eq.TA.3}{{5.13}{60}}
\newlabel{Eq.TA.4}{{5.14}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Weight Stabilization using Reward-Modulated Competitive Synaptic Equilibrium (R-CSE)}{61}{}\protected@file@percent }
\newlabel{Eq.WS.1}{{5.15}{62}}
\newlabel{Eq.WS.3}{{5.16}{63}}
\newlabel{Eq.WS.3.1}{{5.17}{63}}
\citation{AFL1}
\newlabel{Eq.WS.4}{{5.18}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Federated Learning for Consensus Flying}{64}{}\protected@file@percent }
\citation{AFL2}
\citation{AFL3}
\newlabel{Eq.AFL.1}{{5.19}{65}}
\newlabel{Eq.AFL.2}{{5.20}{66}}
\newlabel{Eq.AFL.3}{{5.21}{66}}
\newlabel{Eq.AFL.4}{{5.22}{66}}
\newlabel{Eq.AFL.5}{{5.23}{67}}
\newlabel{Eq.AFL.6}{{5.24}{67}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces High-Level Algorithm for the Proposed \gls {fl} Algorithm}}{68}{}\protected@file@percent }
\newlabel{Algorithm.1}{{3}{68}}
\citation{R1}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results and Discussion}{69}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Simulation without FL}{69}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Simulation Parameters}}{70}{}\protected@file@percent }
\newlabel{table:simulation-parameters}{{5.2}{70}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Parameter values for \gls {lif} neuron model \cite  {R1}}}{70}{}\protected@file@percent }
\newlabel{table:Parameter values for the LIF neuron model}{{5.1}{70}}
\citation{FACL1}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The measured distances during the test for evaluating the performance and detecting the collisions.}}{71}{}\protected@file@percent }
\newlabel{fig:distances}{{5.4}{71}}
\newlabel{eq:actor_output}{{5.25}{71}}
\newlabel{eq:actor_strength}{{5.26}{72}}
\newlabel{eq:value_function}{{5.27}{72}}
\newlabel{eq:TD}{{5.28}{72}}
\newlabel{eq:actor_update}{{5.29}{72}}
\newlabel{eq:critic_update}{{5.30}{73}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Simulation Parameters for the FACL Algorithm}}{73}{}\protected@file@percent }
\newlabel{tab:FACL}{{5.3}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Distances during the test phase (\gls {RCSE} method)}}{74}{}\protected@file@percent }
\newlabel{fig_3_0}{{5.5}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Agents' trajectory during the test phase (\gls {RCSE} method)}}{74}{}\protected@file@percent }
\newlabel{fig_3_1}{{5.6}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Change in commanded distance during the test phase (\gls {RCSE} method)}}{74}{}\protected@file@percent }
\newlabel{fig_3_2}{{5.7}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Distances during the test phase (FACL method)}}{74}{}\protected@file@percent }
\newlabel{fig_3_3}{{5.8}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Agents' trajectory during the test phase (FACL method)}}{74}{}\protected@file@percent }
\newlabel{fig_3_4}{{5.9}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Change in commanded distance during the test phase (FACL method)}}{74}{}\protected@file@percent }
\newlabel{fig_3_5}{{5.10}{74}}
\newlabel{Eq.R-1}{{5.31}{75}}
\newlabel{Eq.R-2}{{5.32}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Agents' path after reward change (test phase)}}{76}{}\protected@file@percent }
\newlabel{fig_3_6}{{5.11}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Distances after reward change (test phase)}}{76}{}\protected@file@percent }
\newlabel{fig_3_7}{{5.12}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Synaptic Weights before Reward change}}{77}{}\protected@file@percent }
\newlabel{fig_3_8}{{5.13}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Synaptic Weights after Reward change}}{77}{}\protected@file@percent }
\newlabel{fig_3_9}{{5.14}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Synaptic weights increase after reward change}}{78}{}\protected@file@percent }
\newlabel{fig_3_10}{{5.15}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Synaptic weights decrease after reward change}}{78}{}\protected@file@percent }
\newlabel{fig_3_11}{{5.16}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Simulation with FL and R-CSE}{78}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Distances during the test phase before reward change in the proposed aggregation method.}}{79}{}\protected@file@percent }
\newlabel{fig_3-7}{{5.17}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Distances during the test phase after reward change in the proposed aggregation method.}}{79}{}\protected@file@percent }
\newlabel{fig_3-8}{{5.18}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Frobenius norm of the agents during the learning phase. The reward changes for the Leader after 600 s.}}{80}{}\protected@file@percent }
\newlabel{fig:3-9}{{5.19}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Communication times for agents and the Central Server (Leader). Red and blue dots show the times that agents and the Central Server have sent their model, respectively.}}{81}{}\protected@file@percent }
\newlabel{fig:3-10}{{5.20}{81}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Comparative Performance Analysis of the Proposed Aggregation Algorithm+\gls {RCSE}, \gls {RCSE}, and FACL}}{81}{}\protected@file@percent }
\newlabel{tab:Results analysis}{{5.4}{81}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Conclusion}{82}{}\protected@file@percent }
\@setckpt{5_FL_with_SNN}{
\setcounter{page}{83}
\setcounter{equation}{32}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{20}
\setcounter{table}{4}
\setcounter{Maxaffil}{2}
\setcounter{authors}{0}
\setcounter{affil}{0}
\setcounter{r@tfl@t}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{3}
\setcounter{ALG@line}{22}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{nlinenum}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{87}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{0}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{0}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
}
