\chapter{Introduction}

\ac{fl} is a groundbreaking approach to machine learning that allows models to be trained directly on edge devices without centralizing data. This method addresses significant concerns regarding data privacy and security by keeping sensitive information local to the device it originates from, thus preventing data leakage and enhancing user privacy. \ac{fl} represents a shift towards decentralized, privacy-preserving machine learning models, making it a critical area of research for advancing machine learning technologies that are both accessible and secure.

\begin{figure}[H]
    \centering 
    \includegraphics{Figures/Schematic of Federated Learning.pdf}
    \caption{Schematic diagram to illustrate distributed learning and \ac{fl} \cite{Ch_1_R1}.}
    \label{Sch_FL}
\end{figure}

Figure \ref{Sch_FL} shows a diagram of distributed learning and \ac{fl}. In conventional learning, the data is collected from the clients, and the model is trained offline. In distributed learning, the dataset is collected at a single data store, and then it is distributed across multiple worker nodes for training. In \ac{fl}, the training is performed at the client, and model gradients are communicated to the central server, which aggregates the gradients, updates the global model, and broadcasts it back to all the clients. Each of these processes is iterative, and models for the clients are periodically updated.

\ac{snn}s bring a new dimension to the capabilities of \ac{fl}. Inspired by the biological processes of the human brain, \ac{snn}s operate on an event-driven basis, processing information only in response to stimuli. This method of operation significantly reduces the power consumption of these networks, making them particularly suited for deployment across distributed, battery-operated devices. The integration of \ac{snn}s with \ac{fl} leverages these unique advantages, combining energy efficiency with the privacy-preserving features of \ac{fl} to address the challenges faced by traditional artificial neural network models in federated settings \cite{Ch_1_R2, Ch_1_R3}.

The energy-efficient nature of \ac{snn}s aligns perfectly with the objectives of \ac{fl}, where models are trained across a network of distributed devices without centralized data collection. Unlike conventional artificial neural networks that require continuous data flow and computation, \ac{snn}s' event-driven operation allows for significant reductions in energy consumption, which is critical for battery-operated agents participating in \ac{fl}. This characteristic of \ac{snn}s supports the development of low-power, distributed learning solutions, enabling more agents to participate in \ac{fl} without compromising on power efficiency \cite{Ch_1_R4}.

Furthermore, the integration of \ac{snn}s in \ac{fl} scenarios can also help overcome limitations associated with bandwidth-constrained environments. The sparse nature of data representation and communication in \ac{snn}s means that less information needs to be exchanged between devices and the central server during the training process. This efficiency is crucial in \ac{fl} environments, where network bandwidth and connectivity can significantly impact the feasibility and performance of distributed learning systems \cite{Ch_1_R5}.

Another critical advantage of \ac{snn}s in the context of \ac{fl} is their compatibility with neuromorphic hardware. Neuromorphic chips, designed to replicate the neural structures of the human brain, provide an ideal platform for deploying \ac{snn}s. This synergy between neuromorphic computing and \ac{snn}s paves the way for the development of highly efficient, scalable, and adaptive \ac{fl} systems capable of leveraging the full potential of edge computing \cite{Ch_1_R6}.

Despite these advantages, the integration of \ac{snn}s with \ac{fl} presents several challenges. The primary hurdle is the complexity of training \ac{snn}s in complex problems. The dynamic and temporal nature of \ac{snn}s introduces new challenges in developing effective \ac{fl} protocols that can accommodate the unique online learning rules of \ac{snn}s. Proper scheduling of communication rounds within the local \ac{snn} time steps is essential for successful collaborative training \cite{Ch_1_R7}.

Moreover, managing the performance trade-offs associated with the frequency of communication rounds in \ac{fl} is another significant challenge. Experiments have shown the impact of the number of time steps between local updates and the frequency of model aggregation on the training performance of \ac{snn}s. Finding an optimal balance to maximize model performance while minimizing communication overhead is crucial for the efficient deployment of \ac{fl} systems powered by \ac{snn}s \cite{Ch_1_R8}.

Additionally, communication constraints and the non-stationarity of data distribution pose significant challenges. The need for larger model update intervals to reduce communication costs and the changing nature of data over time (e.g., reward function in RL) require innovative solutions to maintain the accuracy and reliability of \ac{fl} systems employing \ac{snn}s \cite{Ch_1_R9}.

Despite these challenges, the potential benefits of integrating \ac{snn}s with \ac{fl} justify continued research and development in this area. The combination of energy efficiency, privacy preservation, and compatibility with neuromorphic hardware, along with the distributed and collaborative nature of \ac{fl}, represents a compelling approach to deploying machine learning models in real-world applications \cite{Ch_1_R10}.


\section{Spiking Neural Networks: Models and Learning Algorithms}

\subsection{Neuron model}

In the study of computational neuroscience and the development of neural networks, particularly \ac{snn}, various neuron models have been proposed to simulate the electrical activity of neurons. These models range from simple to complex, aiming to capture the essential features of neuronal dynamics. This section overviews several key neuron models and their characteristics.


\subsubsection{Leaky-Integrate and Fire model}
The \ac{lif} model is a biological model that can be represented as a circuit with a resistor and capacitor and represents a first-order dynamic system \cite{Ch_1_R13},

\begin{equation} \label{Eq.1}
    R_{m}C_{m}\frac{dV_m\left(t\right)}{dt} = E_{l} - V_m\left(t\right) + R_{m}I\left(t\right)
\end{equation}

\noindent where $V_{m}(t)$ is the neuron's membrane potential, $R_{m}$ is the membrane resistance, $C_{m}$ is the membrane capacitance, $E_{l}$ is the resting potential, and $I(t)$ is the input current. The neuron spikes when its potential reaches the threshold potential ($V_{th}$). The potential of the neuron immediately reaches the reset potential ($V_{res}$) after it spikes.

The spike rate is a parameter that determines how fast the neuron spikes \cite{Ch_1_R11}.

\begin{equation} \label{Eq.4}
    r_{[Hz]} = \frac{1}{t_{isi}\enspace [s]}
\end{equation}

\noindent where $t_{isi}$ is the inter-spike interval that can be calculated using the neuron model, when the potential of a neuron reaches the threshold potential, it fires. Therefore, based on the analytical solution of (\ref{Eq.1}), the inter-spike interval time can be written as,

\begin{equation} \label{Eq.7}
    t_{isi} = \tau_{m}\ln\left(\frac{E_{l} + R_{m}I - V_{res}}{E_{l} + R_{m}I - V_{th}}\right)
\end{equation}

\noindent where $\tau_{m}$ is the membrane time constant. 

According to (\ref{Eq.7}), the following condition should be satisfied to have a finite value for $t_{isi}$,

\begin{equation} \label{Eq.8}
    E_{l} + R_{m}I - V_{th} > 0
\end{equation}

\noindent or

\begin{equation} \label{Eq.9}
    I > \frac{V_{th} - E_{l}}{R_{m}}
\end{equation}

\noindent which means that the input current higher than the above value generates spikes. 

After calculating the minimum input for neurons, we must find the maximum input based on the inter-spike interval. Equation (\ref{Eq.7}) can be written as,

\begin{equation} \label{Eq.10}
    t_{isi} = \tau_{m}\ln\left(1+\frac{V_{th} - V_{res}}{E_{l} + R_{m}I - V_{th}}\right)
\end{equation}

\noindent Equation (\ref{Eq.10}) can be approximated using the Maclaurin series for the natural logarithm function ($\ln(1+z)\approx z$) as follows,

\begin{equation} \label{Eq.11}
    t_{isi} = \frac{\tau_{m}\left(V_{th} - V_{res}\right)}{E_{l} + R_{m}I - V_{th}}
\end{equation}

\noindent Solving for $I$, an input current as a function of the inter-spike interval can be obtained,

\begin{equation} \label{Eq.12}
    I = \frac{\tau_{m}\left(V_{th} - V_{res}\right)}{t_{isi}R_{m}} + \frac{V_{th} - E_{l}}{R_{m}}
\end{equation}

The maximum value for the input current makes the neuron fire at each sample time ($\Delta t$). Therefore, the maximum input current is,

\begin{equation} \label{Eq.13}
    I^{max} = \frac{\tau_{m}\left(V_{th} - V_{res}\right)}{\Delta t R_{m}} + \frac{V_{th} - E_{l}}{R_{m}}
\end{equation}

In this section, we obtained the minimum and maximum values for input current using (\ref{Eq.9}) and (\ref{Eq.13}). These equations are used in the learning and encoding processes of the \ac{snn}.

Following our exploration of the Leaky Integrate-and-Fire (LIF) model, which provides a simplified framework for simulating the basic properties of neuronal firing, we now introduce other biologically-inspired models taht incorporate the dynamics of ionic currents and more complex neuronal behaviors.

\subsubsection{Other Neuron Models}

The Hodgkin-Huxley model is a foundational framework in neuroscience that meticulously describes the ionic processes critical for the initiation and propagation of action potentials in neurons~\cite{Ch_1_R12}. The model uses differential equations to capture the dynamics of membrane potential (\(V\)) influenced by various ion-specific currents and gating variables (\(n, m, h\)), which regulate ion channel states. These gating variables, which transition between 0 and 1, reflect the proportion of ion channels in different states, crucial for mimicking the action potentials’ temporal complexity. In contrast, the FitzHugh-Nagumo model simplifies the complex Hodgkin-Huxley framework into a two-variable system, focusing on capturing the essential characteristics of neuronal excitability with fewer computational demands. It models the membrane potential (\(V\)) and incorporates a recovery variable (\(U\)) to represent the system's response dynamics, with external current (\(I_{ext}\)) as a driving factor. This model adjusts the recovery dynamics through parameters \(a\) and \(b\) and a time constant \(\tau\), streamlining the depiction of excitable systems while maintaining a balance between biological fidelity and computational efficiency~\cite{Ch_1_R14}. 

The Izhikevich model merges the biological realism of the Hodgkin-Huxley model with the computational simplicity of integrate-and-fire models, offering a balanced approach for simulating neuronal behavior. It simplifies the description of neuronal dynamics to two main variables: the membrane potential (\(v\)) and a recovery variable (\(u\)), which are adjusted through a set of parameters (\(a, b, c, d\)) that control the model’s response to inputs and recovery processes~\cite{Ch_1_R15}. This model is particularly noted for its ability to produce diverse firing patterns with computational efficiency, making it suitable for simulating large networks of neurons while still capturing key aspects of neuronal activity.

\subsection{Learning Approaches in SNNs}

\subsection*{Hebbian Learning}


Hebbian Learning is a fundamental neural learning principle summarized by the axiom ``neurons that fire together, wire together," describing how simultaneous activation of neurons leads to strengthened connections between them~\cite{Ch_1_R16}. Hebbian learning, particularly within the context of \ac{snn}, primarily revolves around the modulation of synaptic strengths based on the firing rates of pre- and postsynaptic neurons. The principles of locality and joint activity are fundamental, emphasizing that synaptic changes occur only when both neurons are active simultaneously.

\textbf{Methods of Synaptic Modification:}
\begin{itemize}
    \item \textbf{Local Rules:} Synaptic changes are influenced directly by the activities of the connecting neurons without external influences.
    \item \textbf{Bounded Growth:} To avoid uncontrolled increases in synaptic strength, models typically incorporate mechanisms such as hard and soft bounds. Hard bounds prevent any further increase once a maximum weight is achieved, while soft bounds slow down the rate of increase as the maximum is approached~\cite{Ch_1_R25}.
    \item \textbf{Synaptic Decay:} Realistic models also consider mechanisms for reducing synaptic strengths, typically through a decay term that weakens connections in the absence of activity~\cite{Ch_1_R17}.
\end{itemize}

\textbf{Advanced Hebbian Models:}
\begin{itemize}
    \item \textbf{Covariance Rule:} This model refines the synaptic modification to depend on the deviation of firing rates from their means, enhancing the dynamic response of synapses to changes in neural activity~\cite{Ch_1_R32}.
    \item \textbf{Oja's Rule:} A self-stabilizing rule that ensures synaptic weights do not grow indefinitely by normalizing the weight vector, thereby maintaining the overall stability of the network~\cite{Ch_1_R19}.
    \item \textbf{BCM Rule:} The Bienenstock-Cooper-Munro rule introduces an adaptive threshold for synaptic modification, which evolves based on the historical activity of the neuron, allowing for more refined potentiation and depression based on relative activity levels.~\cite{Ch_1_R31}.
    \item \textbf{RCHP:} Rarely Correlating Hebbian Plasticity focuses on synaptic changes driven by rare, significant coincidences in neuronal activity, aiming to strengthen connections that are crucial for neural function while avoiding over-strengthening due to common activity patterns~\cite{Ch_1_R20}.
\end{itemize}

\textbf{Incorporation into Reinforcement Learning:}
Modern adaptations in \ac{snn}s integrate the concept of rewards, adding a third dimension to synaptic adjustments. This integration uses scaling or gating mechanisms in response to global reward signals, further refining the learning capabilities of neural networks based on external feedback.


\subsection*{Neo-Hebbian Learning and Modulation Mechanisms}

In neo-Hebbian reinforcement learning, significant advancements come from a global reward signal modulating synaptic plasticity alongside an eligibility trace that decays over time~\cite{Ch_1_R21}. This trace increases with recent, successful neurotransmission and decreases as time passes, linking closely with Temporal-Difference (TD) learning mechanisms. Such dynamics allow for Long-Term Potentiation (LTP) or Depression (LTD) at synapses based on the timing of synaptic activity and the nature of the reward signal. Specifically, recent successful activities followed by positive rewards enhance LTP, while activities preceding negative rewards lead to LTD.


\textbf{Distal rewards and credit assignment} 

This model introduces a sophisticated approach to synaptic modification through the interaction of \ac{stdp} with a modulatory signal reflective of reward, embodying the essence of TD learning within the realm of spiking neurons.

Central to this framework is the eligibility trace mechanism, elegantly adapted from its conventional application in TD learning to facilitate synaptic credit assignment over varying temporal extents. This adaptation allows for the dynamic modulation of synaptic strengths based on the timing and sequence of pre- and post-synaptic spikes, in conjunction with the temporal dynamics of received rewards. The eligibility trace is mathematically represented as follows~\cite{Ch_1_R22}:

\begin{equation}
    \frac{dC_{ji}}{dt} = -\frac{C_{ji}}{\tau_{C}} + \text{STDP}(t_{\text{post}} - t_{\text{pre}})\delta(t - t^{(f)})
\end{equation}

Here, $C_{ji}(t)$ denotes the eligibility trace for the synapse between pre-synaptic neuron $j$ and post-synaptic neuron $i$, evolving with a decay governed by $\tau_{C}$. The Dirac delta function, $\delta(t - t^{(f)})$, signifies the occurrence of a spike, serving as a pivotal factor in the temporal credit assignment process. 

The STDP function can be represented as follows,

\begin{equation}
    STDP(\tau) = \mathcal{A} \exp\left(-\frac{\tau}{\tau_{s}}\right) \text{for \(\tau \geq 0\)}, (\tau = t_{\text{post}} - t_{\text{pre}})
\end{equation}

\noindent where \(\mathcal{A}\) stands as the amplitude and \(\tau_{s}\) acts as the time constant.

Synaptic weight updates are then guided by the interaction between the eligibility trace and the reward signal, $R(t)$, as captured in the following equation:

\begin{equation}
    \frac{dw_{ji}}{dt} = C_{ji}(t) \cdot R(t)
\end{equation}

\noindent where $dw_{ji}/dt$ symbolizes the rate of change in synaptic weight, contingent upon the compounded influence of the eligibility trace and the reward signal.

\textbf{Hypothesis Testing Plasticity (HTP)}

The HTP model focuses on the sophisticated evolution of synaptic strengths through the interaction with both short-term and long-term components of synaptic weights~\cite{Ch_1_R23}. The HTP model delineates the dynamics of synaptic strength through a dual-component approach. This approach separates synaptic weight into short-term and long-term components. In the following, the mathematical representation of this model, including the dynamics of short-term synaptic changes and the conditions for long-term consolidation, is discussed.

Short-term synaptic weight changes are governed by,

\begin{equation}
\dot{w}_{ji}^{st}(t) = -\frac{w_{ji}^{st}(t)}{\tau_{st}} + M(t) \cdot \text{RCHP}_{ji}(t)
\end{equation}

\noindent where $M(t)$, representing the dynamic concentration of reward, evolves as:

\begin{equation}
\dot{M}(t) = -\frac{M(t)}{\tau_{M}} + \alpha R(t) - b
\end{equation}

Long-term synaptic changes are then formulated as:

\begin{equation}
\dot{w}_{ji}^{lt}(t) = \beta \mathcal{H}(w_{ji}^{st}(t) - \Phi)
\end{equation}

\noindent where $\mathcal{H}$ denotes the Heaviside step function, introducing a binary condition for the consolidation of synaptic changes based on the magnitude of short-term synaptic weight relative to the threshold $\Phi$, and $\beta$ represents the consolidation rate.

This framework integrates the dynamics of synaptic plasticity with mechanisms of reward-based learning, offering a comprehensive model for understanding the biological and computational foundations of learning and memory. The choice of the R-STDP algorithm for this thesis is underpinned by several compelling reasons:

\begin{enumerate}
    \item \textbf{Biological Plausibility}: R-STDP closely mimics biological learning processes by integrating synaptic plasticity with reward signals. This alignment with natural neural mechanisms enhances the potential for developing more realistic and efficient neuromorphic systems.
    
    \item \textbf{Temporal Precision}: Unlike traditional Hebbian learning, R-STDP leverages precise spike-timing information, allowing for fine-grained synaptic modifications. This temporal specificity is crucial for tasks that require high temporal resolution, such as sensory processing and motor control.
    
    \item \textbf{Robustness to Noise}: By focusing on the timing of spikes rather than just firing rates, R-STDP can be more robust to noise and fluctuations in neural activity. This robustness is particularly advantageous in real-world scenarios where sensory inputs and neural responses can be highly variable.
    
    \item \textbf{Credit Assignment}: The eligibility trace mechanism in R-STDP provides an elegant solution to the credit assignment problem, distributing synaptic changes over time based on the timing of spikes and rewards. This feature supports more effective learning in complex, temporally extended tasks.
    
    \item \textbf{Adaptive Learning}: R-STDP’s capability to modulate synaptic changes based on the reward history allows for continuous adaptation and refinement of synaptic connections. This adaptive learning is essential for long-term performance and generalization in changing environments.
\end{enumerate}

Given these advantages, the R-STDP algorithm offers a balanced approach that combines the strengths of Hebbian learning with the benefits of reinforcement learning. Its ability to integrate temporal dynamics, reward signals, and synaptic plasticity makes it a suitable choice for advancing the state-of-the-art in neuromorphic computing and autonomous systems.

\section{Federated Learning}


A cornerstone of \ac{fl} is the aggregation method, which synthesizes model updates from clients to improve a global model. This section delves into the main aggregation algorithms in \ac{fl}, highlighting their operational principles and underlying equations.

\subsection*{FedAvg}

FedAvg, or Federated Averaging, is the seminal aggregation method in \ac{fl}. It operates by averaging the weights of models updated locally by clients. The global model update in FedAvg is mathematically represented as~\cite{Ch_1_R26}:

\begin{equation}
    w_{\text{glob}}^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} w_k^{t+1}
\end{equation}

\noindent where $w_{\text{glob}}^{t+1}$ denotes the global model parameters after aggregation, $n_k$ is the number of data points at client $k$, $n$ is the total number of data points across all clients, and $w_k^{t+1}$ represents the parameters of the model updated locally by client $k$.

\subsection*{FedProx}

FedProx addresses heterogeneity issues in client data distributions by introducing a proximal term to the local optimization problem. This approach moderates the deviation of local models from the global model. The local update rule in FedProx is formulated as~\cite{Ch_1_R27}:

\begin{equation}
    w_{\text{glob}}^{t+1} = \min_{w} \mathcal{L}_k(w) + \frac{\mu}{2}\|w - w_{\text{glob}}^t\|^2
\end{equation}

\noindent where $\mathcal{L}_k(w)$ is the loss on client $k$, $w_{\text{glob}}^t$ is the global model parameters at time $t$, and $\mu$ is a regularization parameter controlling the proximity between local and global models.

\subsection*{Scaffold}

Scaffold addresses the challenges of statistical heterogeneity and client drift in \ac{fl} environments. It restricts client drift and enhances convergence by employing control variates to correct the local updates towards the global objective, effectively steering them closer to the global optimum and reducing the variance in updates caused by data that is not Independent and Identically Distributed (IID) across all clients. The control variate updates are defined as~\cite{Ch_1_R28}:

\begin{equation}
    c_k^{+} = c_k - c + \frac{1}{\eta L}(w_{\text{glob}}^t - w_k^t)
\end{equation}

\noindent where $c_k$ and $c$ represent the control variates for client $k$ and the global model, respectively. Here, $L$ denotes the number of local updates, and $\eta$ is the learning rate. The control variates \(c_k\) for each client are commonly initialized to zero, allowing unbiased initial updates. The global control variate \(c\) is typically chosen by aggregating or averaging these local variates \(c_k\) from all clients, which helps align individual client updates more closely with the global model objective. By introducing control variates on both the server and client sides, Scaffold corrects the direction of local updates before they are aggregated. This ensures consistency among local updates and addresses the slow convergence rates often experienced in traditional \ac{fl} due to client drift. Specifically, the mechanism leverages the discrepancy between local and global control variates to adjust the updates, making Scaffold particularly effective in environments with heterogeneous data distributions.


\subsection*{FedNova}

FedNova introduces a normalization mechanism to adjust the aggregation process according to the number of local updates performed by each client, accommodating non-IID data scenarios more effectively. Its aggregation rule is expressed as~\cite{Ch_1_R29}:

\begin{equation}
    w_{glob}^{t+1} = w_{glob}^t - \tau_{\text{eff}} \sum_{k=1}^{K} \frac{n_k}{n} \cdot \eta d_k^t
\end{equation}

\begin{equation}
    d_k^t = G_k^t a_k^t / \|a_k^t\|_1
\end{equation}

\noindent where $G_k^t$ is the stack of gradients received from client $k$ at round $t$, $a_k^t$ is a non-negative vector representing the local update rate, and $\tau_{\text{eff}}$ is the effective step size.

\subsection*{MOON}

MOON, or Model-contrastive \ac{fl}, minimizes the contrastive loss between the local and global models to preserve model consistency across clients. The contrastive loss is given by:

\begin{equation}
    l_{\text{con}} = -\log \left( \frac{\exp(\text{sim}(w_k^t, w_{\text{glob}}^t) / \tau)}{\exp(\text{sim}(w_k^t, w_{\text{glob}}^t) / \tau) + \exp(\text{sim}(w_k^t, w_k^{\text{prev}}) / \tau)} \right)
\end{equation}

\noindent where $\text{sim}(\cdot)$ denotes the cosine similarity between models, indicating how aligned or similar two model parameter sets are by calculating the cosine of the angle between them. This similarity metric is essential for understanding how closely the local model parameters $w_k^t$ of client $k$ at training iteration $t$ match with the global model parameters $w_{\text{glob}}^t$, as well as how these parameters have evolved from the previous iteration $w_k^{\text{prev}}$. The global model parameters $w_{\text{glob}}^t$ are derived by aggregating updates from all participating clients, aiming to reflect a consensus model that captures learned patterns across the network.

The parameter $\tau$, known as the temperature parameter, plays a critical role in moderating the contrastive loss. It adjusts the sensitivity of the loss function to differences in model similarity, affecting how aggressively the algorithm seeks to align the local and global models. A higher $\tau$ value results in a smoother loss function, potentially making the training process more tolerant to variations among models.

These aggregation methods embody the evolution of \ac{fl} towards accommodating diverse client conditions and data distributions. Each method introduces unique adjustments to the aggregation process, aiming to enhance convergence, reduce communication overhead, or improve robustness against non-IID data and adversarial clients.

\section{Literature Review}

\subsection{Spiking Neural Network}

\ac{snn}s have garnered significant attention for their ability to emulate complex neural dynamics observed in biological systems. This literature review focuses on the learning mechanisms, including STDP, and their implications in modeling biological learning behaviors.

The investigation of competitive Hebbian learning through STDP reveals how the timing of spikes modulates synaptic efficacy, underscoring the significance of temporal dynamics in synaptic modification within neural circuits \cite{Ch2_R9}. This highlights the critical role of spike timing in learning processes, which is essential for understanding neural adaptation.

In a significant study, the distal reward problem was tackled by linking STDP with dopamine signaling, illustrating a novel approach to reward-based learning in the brain \cite{Ch_1_R22}. This study proposed a model where dopamine modulates STDP mechanisms, effectively capturing the essence of reward-based learning processes.

Another research posited that STDP emerges from fundamental learning rules governed by intracellular calcium dynamics, suggesting a deeper biophysical basis for synaptic strength regulation \cite{Ch2_R6}. This challenges the traditional understanding of STDP and points towards a more comprehensive framework for neural plasticity.

An extensive review of the development and implications of STDP in neural circuits provided insights into its critical role in precise spike timing for synaptic modification, enriching our understanding of the neural basis for learning and memory \cite{Ch2_R5}. 

A study on multiagent reinforcement learning within the Iterated Prisoner's Dilemma framework highlighted \ac{snn}s' comparable or enhanced performance against traditional models \cite{Ch2_R4}. This emphasizes the critical role of spike timing in complex decision-making processes, showcasing \ac{snn}s' potential in simulating interactive and competitive environments.

The introduction of a model for parallel path planning using spiking neural activity, inspired by hippocampal navigation strategies, showcased the efficiency of \ac{snn}s in solving spatial navigation problems through biologically plausible mechanisms \cite{Ch2_R3}.

In a pioneering study, the implementation of probabilistic inference within networks of LIF neurons demonstrated how Bayesian networks can be transformed into a computable framework for \ac{snn}s \cite{Ch2_R1}. This methodology underscores \ac{snn}s' capability to perform complex cognitive functions through probabilistic reasoning.

The performance comparison between \ac{snn}s and multilayer perceptrons in a computer-based racing game highlighted \ac{snn}s' superior adaptability and decision-making capabilities in dynamic environments, attributed to the efficient processing of real-time data through spike-based mechanisms \cite{Ch2_R2}.

Hierarchical Bayesian inference within \ac{snn}s introduces a learning algorithm based on synaptic plasticity, showcasing the ability of \ac{snn}s to execute complex tasks like pattern recognition through hierarchical structures \cite{Ch2_R14}. This emphasizes the adaptability and computational efficiency of \ac{snn}s.

BP-STDP, which approximates backpropagation using STDP, bridges the gap between \ac{snn} mechanisms and traditional neural network computational efficiency \cite{Ch2_R16}. This algorithm advances learning algorithms for \ac{snn}s, making them more accessible for machine learning tasks.

Indirect and direct training methods for \ac{snn}s in end-to-end control of a lane-keeping vehicle highlight the effectiveness of both approaches in utilizing \ac{snn}s for real-world control tasks, providing insights into training \ac{snn}s for complex systems like autonomous vehicles \cite{Ch2_R17}.

N3-CPL, a neuroplasticity-based learning method for neuromorphic networks, focuses on cell proliferation in \ac{snn}s, enhancing learning capabilities through mechanisms inspired by biological neural network growth and adaptation \cite{Ch2_R18}.

Parameter optimization in an \ac{snn} model for UAV obstacle avoidance tackles the challenge of tuning \ac{snn} parameters for specific tasks using optimization techniques, emphasizing the importance of optimization in \ac{snn} application for real-time processing and decision-making \cite{Ch2_R19}.

A methodology for multi-task autonomous learning in mobile robots using \ac{snn}s was proposed, employing Modified Integrate-and-Fire and \ac{lif} neuron models, alongside a task switch mechanism inspired by lateral inhibition and a novel learning rule based on \ac{stdp}. This approach demonstrated \ac{snn}s' capabilities in efficiently learning and adapting across various tasks such as obstacle avoidance and target tracking \cite{Ch2_R20}.

An autonomous learning framework that combines \ac{snn}s with a pre-trained binary Convolutional Neural Network for image-based reinforcement learning was developed. This hybrid model efficiently processes high-dimensional sensory data and learns from sparse rewards, highlighting the synergistic potential of \ac{snn}s and CNNs in complex visual environments \cite{Ch2_R21}.

A transfer learning algorithm for \ac{snn}s aimed at deep learning tasks was introduced, utilizing Centered Kernel Alignment for domain distance measurement and focusing on domain-invariant representation. This methodology enables effective knowledge transfer, showcasing the scalability and adaptability of \ac{snn}s \cite{Ch2_R22}.

Mathematical constraints influencing Hebbian learning and STDP in \ac{snn}s were explored, revealing relationships between synaptic weight promotion/demotion probabilities and normalized weights, offering insights into optimizing \ac{snn} training \cite{Ch2_R24}.

``Spikepropamine," incorporating differentiable plasticity within \ac{snn}s, was presented. This approach enables adaptive learning and improved task performance, signifying advancements in neuromorphic computing and robotics applications of \ac{snn}s \cite{Ch2_R25}.

NeuronGPU, a library for simulating large-scale networks of spiking neurons with GPU acceleration, was developed. This tool demonstrated scalability and efficiency in simulating complex neuronal dynamics, marking a significant contribution to computational neuroscience tools \cite{Ch2_R26}.

% \ac{snn} and their learning algorithms have garnered significant attention for their potential to model complex neural dynamics and learning processes observed in biological systems. This literature review explores various facets of \ac{snn}s, focusing on their learning mechanisms, including STDP, and their applications in solving computational tasks that mirror biological learning behaviors.

% The investigation of competitive Hebbian learning through STDP revealed how the timing of spikes modulates synaptic efficacy. This highlighted temporal dynamics' significance in synaptic modification and competitive learning within neural circuits \cite{Ch2_R9}.

% In \cite{Ch_1_R22}, the distal reward problem by linking STDP with dopamine signaling was tackled, illustrating a novel approach to reward-based learning in the brain. The study proposed a model in which dopamine modulated STDP mechanisms, effectively capturing the essence of reward-based learning processes.

% Another research posited that STDP emerges from fundamental learning rules governed by intracellular calcium dynamics. This challenged the primacy of STDP, suggesting a deeper biophysical basis for synaptic strength regulation \cite{Ch2_R6}.

% An examination of rate dynamics in LIF neurons with strong synapses focused on synaptic strength's influence on neural computation. Findings provided insights into synaptic integration and its effects on neurons' computational properties \cite{Ch2_R7}.

% In \cite{Ch2_R5}, an extensive review of the development and implications of STDP in neural circuits was provided. Their work highlighted the critical role of precise spike timing in synaptic modification, enriching our understanding of the neural basis for learning and memory.

% A study on multiagent reinforcement learning within the Iterated Prisoner's Dilemma framework highlighted \ac{snn}s' comparable or enhanced performance against traditional models. This emphasized spike timing's critical role in complex decision-making \cite{Ch2_R4}.

% The introduction of a model for parallel path planning using spiking neural activity was inspired by hippocampal navigation strategies. The model efficiently solved spatial navigation problems through simulations, showcasing the biologically plausible mechanisms of cognitive tasks within \ac{snn}s \cite{Ch2_R3}.

% In a pioneering study, the implementation of probabilistic inference within LIF neuron networks was demonstrated, illustrating the transformation of Bayesian networks into a computable framework for \ac{snn}s. The methodology involved the adaptation of Bayesian networks to facilitate computation by \ac{snn}s, with findings highlighting \ac{snn}s' capability to perform complex cognitive functions through probabilistic reasoning \cite{Ch2_R1}.

% The performance comparison between \ac{snn}s and multilayer perceptrons in a computer-based racing game showed \ac{snn}s' superior adaptability and decision-making capabilities in dynamic environments. This was attributed to the efficient processing of real-time data through spike-based mechanisms \cite{Ch2_R2}.


% %% ------------------------------------------------------------

% A spiking neural model aimed at adaptive arm control integrates structures and mechanisms from the motor cortex and cerebellum, mapped onto a simulated three-link arm. Utilizing a stability framework, it dynamically adapts to arm dynamics and kinematic structure changes, such as growth or movement through different media \cite{Ch2_R10}. This model demonstrates the potential of \ac{snn}s in replicating complex motor control tasks, suggesting their applicability in both understanding biological systems and robotics.

% Hierarchical Bayesian inference within \ac{snn}s introduces a learning algorithm based on synaptic plasticity, showcasing the ability of \ac{snn}s to execute complex tasks like pattern recognition through hierarchical structures \cite{Ch2_R14}. This emphasizes the adaptability and computational efficiency of \ac{snn}s.

% A bio-inspired \ac{snn} framework for nonlinear system control focuses on UAV obstacle avoidance, employing a hierarchical learning approach through \ac{stdp} \cite{Ch2_R11}. This network effectively processes visual data for real-time control in robotics, showcasing the significance of \ac{snn}s in neuromorphic computing applications.

% Generalized leaky integrate-and-fire (GLIF) models classify multiple neuron types by fitting these models to a diverse set of electrophysiological data \cite{Ch2_R12}. This work demonstrates the capability of simple neuron models to represent complex neural dynamics, providing tools for neural circuit studies.

% First-spike-based visual categorization employs \ac{stdp} in \ac{snn}s, highlighting their efficiency in pattern recognition tasks by leveraging the temporal dynamics of spikes \cite{Ch2_R13}. This approach offers a promising method for neuromorphic hardware implementations.

% Synaptic-dependent spike latency in \ac{snn}s for UAV control employs spiking activity simulations in response to environmental stimuli for efficient path planning \cite{Ch2_R15}. This study exemplifies \ac{snn} applications in navigation and control, highlighting their potential in real-time decision-making for autonomous systems.

% BP-STDP, approximating backpropagation using STDP, bridges the gap between \ac{snn} mechanisms and traditional neural network computational efficiency \cite{Ch2_R16}. This algorithm advances learning algorithms for \ac{snn}s, making them more accessible for machine learning tasks.

% Indirect and direct training methods for \ac{snn}s in end-to-end control of a lane-keeping vehicle highlight the effectiveness of both approaches in utilizing \ac{snn}s for real-world control tasks, providing insights into training \ac{snn}s for complex systems like autonomous vehicles \cite{Ch2_R17}.

% N3-CPL, a neuroplasticity-based learning method for neuromorphic networks, focuses on cell proliferation in \ac{snn}s, enhancing learning capabilities through mechanisms inspired by biological neural network growth and adaptation \cite{Ch2_R18}.

% Parameter optimization in an \ac{snn} model for UAV obstacle avoidance tackles the challenge of tuning \ac{snn} parameters for specific tasks using optimization techniques, emphasizing the importance of optimization in \ac{snn} application for real-time processing and decision-making \cite{Ch2_R19}.

% A methodology for multi-task autonomous learning in mobile robots using \ac{snn}s was proposed in \cite{Ch2_R20}. Modified Integrate-and-Fire and \ac{lif} neuron models were employed, alongside a task switch mechanism inspired by lateral inhibition and a novel learning rule based on \ac{stdp}. This approach demonstrated \ac{snn}s' capabilities in efficiently learning and adapting across various tasks such as obstacle avoidance and target tracking.

% An autonomous learning framework that combines \ac{snn}s with a pre-trained binary Convolutional Neural Network for image-based reinforcement learning was developed in \cite{Ch2_R21}. This hybrid model efficiently processes high-dimensional sensory data and learns from sparse rewards, highlighting the synergistic potential of \ac{snn}s and CNNs in complex visual environments.

% A transfer learning algorithm for \ac{snn}s aimed at deep learning tasks was introduced by \cite{Ch2_R22}, utilizing Centered Kernel Alignment for domain distance measurement and focusing on domain-invariant representation. This methodology enables effective knowledge transfer, showcasing the scalability and adaptability of \ac{snn}s.

% The integration of \ac{snn}s within wireless communication systems, focusing on energy-efficient, distributed processing, was investigated in \cite{Ch_1_R4}. Through \ac{fl} and neuromorphic sensing, the potential for \ac{snn}s to enhance low-power edge intelligence and communication was demonstrated.

% Mathematical constraints influencing Hebbian learning and STDP in \ac{snn}s were explored in \cite{Ch2_R24}. The analysis revealed relationships between synaptic weight promotion/demotion probabilities and normalized weights, offering insights into optimizing \ac{snn} training.

% ``Spikepropamine," incorporating differentiable plasticity within \ac{snn}s, was presented in \cite{Ch2_R25}. This approach enables adaptive learning and improved task performance, signifying advancements in neuromorphic computing and robotics applications of \ac{snn}s.

% NeuronGPU, a library for simulating large-scale networks of spiking neurons with GPU acceleration, was developed in \cite{Ch2_R26}. This tool demonstrated scalability and efficiency in simulating complex neuronal dynamics, marking a significant contribution to computational neuroscience tools.

% HybridSNN, an architecture blending shallow \ac{snn}s with machine learning techniques for adaptive deep learning, was proposed by \cite{Ch2_R27}. The effectiveness of this architecture in complex pattern recognition tasks was showcased through data-driven optimization and a novel network structure.

% The integration of STDP learning with binary networks for image-based reinforcement learning was explored by \cite{Ch2_R28}. This methodology enabled efficient learning from high-dimensional inputs and sparse rewards, highlighting the utility of \ac{snn}s in reinforcement learning frameworks.

% QC\_SANE, a robust DRL framework utilizing a quantile critic with a spiking actor and normalized ensemble, was introduced in \cite{Ch2_R29}. This approach significantly enhanced performance and robustness in continuous control problems, demonstrating the potential of \ac{snn}s in DRL applications.


% A novel biological-like controller utilizing enhanced \ac{snn}s aimed at emulating human arm control was introduced \cite{Ch2_R30}. The incorporation of a novel synapse model simulating biological synapses with multiple channels significantly enhances the realism of synaptic interactions. Through supervised STDP learning, the model effectively adapts and learns from dynamic changes, showcasing efficient control over arm movements.

% Another study presented a control algorithm for multiagent cooperation using \ac{snn}s, leveraging the Izhikevich model of spiking neurons \cite{Ch2_R31}. The algorithm's capability for dynamic restructuring of the neural network in real-time facilitates adaptive cooperative behavior among agents. This control algorithm exhibited efficient cooperation through simulations, marking an advancement over traditional neural network approaches in dynamic environments.

% The development of a brain-inspired approach for collision-free movement planning in small operational spaces using \ac{snn}s was also documented \cite{Ch2_R32}. By integrating the \ac{lif} neuron model with a dynamic learning mechanism, the \ac{snn} successfully simulates complex movement planning and adaptation, validating its applicability to tasks requiring precise control and collision avoidance.

% Deep Spiking Q-Networks (DSQNs) were introduced for human-level control in reinforcement learning tasks, incorporating back-propagated error signals into an STDP-based learning framework \cite{Ch2_R33}. DSQNs achieve high performance on Atari game simulations, demonstrating the potential of directly trained \ac{snn}s for complex decision-making tasks.

% Continuous learning in \ac{snn}s trained with local rules to mitigate catastrophic forgetting (CF) was explored, proposing architecture-based methods, memory replay, and regularization techniques \cite{Ch2_R34}. These methods effectively reduce CF, highlighting the potential of \ac{snn}s for continuous and adaptive learning.

% Investigations into STDP with activation-dependent scaling for receptive field development in \ac{snn}s showcased the modified STDP rule's enhancement of the network's autonomous development of receptive fields \cite{Ch2_R35}. This improvement in classification accuracy on the MNIST dataset underscores the importance of adaptive learning rules in optimizing \ac{snn} performance for pattern recognition tasks.

% A focus on sequence learning in \ac{snn}s modeled on hippocampal circuitry achieved learning in a single trial \cite{Ch2_R36}. Inspired by the brain's hippocampal structure, the network architecture demonstrates effective sequence learning capabilities, offering insights into how biological principles can enhance learning efficiency in neural networks.

% A learning algorithm modulating STDP with back-propagated error signals for audio classification tasks was developed \cite{Ch2_R37}. This study demonstrates the feasibility of training \ac{snn}s for complex auditory pattern recognition, achieving competitive performance and marking a significant advancement in training \ac{snn}s for real-world applications.

% Meta neurons were introduced to improve \ac{snn}s for efficient spatio-temporal learning \cite{Ch2_R38}. Incorporating second-order dynamics and a recovery variable into neuron models, the \ac{snn}s exhibit enhanced performance across various learning tasks, highlighting the potential of advanced neuron models in capturing complex patterns and improving \ac{snn}s' generalization ability.


\subsection{Federated Learning}

\ac{snn}s offer a promising alternative to conventional ANN for the implementation of on-device low-power online learning and inference. On-device training is, however, constrained by the limited amount of data available on each device.

The limitations imposed by on-device training of \ac{snn}s, due to the constrained data availability on each device, is studied in \cite{Ch_1_R2}. Based on the results, the limitation can be mitigated through cooperative training utilizing \ac{fl}. An online FL-based learning rule, termed FL-SNN, for networked on-device SNNs is introduced. Through this scheme, local feedback signals are utilized within each SNN instead of backpropagation, and global feedback is facilitated through communication via a base station, showcasing the potential to significantly enhance training over isolated approaches by enabling a flexible compromise between communication load and accuracy through selective synaptic weight exchange.

A novel approach of selective model aggregation in \ac{fl} within the context of Vehicular edge computing (VEC) is explored in \cite{Ch2_R40}. This approach focuses on the unique challenges posed by the diversity in image quality and computational capacity of vehicular clients for image classification. The approach, which selectively aggregates `fine' local DNN models based on an evaluation of local image quality and computation capability without compromising client privacy, employs two-dimensional contract theory for model selection due to the central server's unawareness of these local parameters. This methodology is shown to outperform conventional federated averaging in terms of accuracy and efficiency on the MNIST and BelgiumTSC datasets, also offering higher utility at the central server.

The paper by \cite{Ch2_R41} discusses a novel approach for fast global model aggregation in \ac{fl} via over-the-air computation, aiming to address the primary bottleneck of limited communication bandwidth in the aggregation of locally computed updates across devices with strict latency and privacy requirements, like drones and smart vehicles. This approach, employing a sparse and low-rank optimization problem solution through a DC algorithm for joint device selection and beamforming design, demonstrates significant algorithmic advantages and performance improvements in numerical results.

According to \cite{Ch2_R42}, \ac{fl} is suggested as a solution to the challenges faced in training machine learning models on IoT data due to constraints in network bandwidth, storage, and privacy. The focus is on how existing work has primarily centered on learning algorithms' convergence time, leaving issues like incentive mechanisms largely unexplored. A deep reinforcement learning-based incentive mechanism is proposed to motivate edge nodes towards model training contribution, with its efficiency validated through numerical experiments.

In \cite{Ch_1_R4}, the exploration of synergies between wireless communications and artificial intelligence, particularly the challenges of implementing ML models on battery-powered devices connected via bandwidth-constrained channels, is discussed. The paper highlights how \ac{fl} for distributed training of SNNs and the integration of neuromorphic sensing with SNNs can help overcome these challenges.

The research presented in \cite{Ch_1_R1} introduces a \ac{fl} method designed for training decentralized and privacy-preserving SNNs, focusing on the energy efficiency advantages of SNNs in \ac{fl} scenarios across energy-constrained devices. The method's effectiveness is experimentally validated through improved accuracy and energy efficiency in comparison to ANNs on CIFAR10 and CIFAR100 benchmarks.

A comprehensive study on deploying distributed learning over wireless edge networks, highlighting the challenges and emerging paradigms like \ac{fl} and distributed inference, is provided in \cite{Ch2_R43}. It presents an overview of communication techniques for efficient deployment and future research opportunities within wireless networks.

The effect of user selection and resource allocation on the convergence time and accuracy of \ac{fl} over wireless networks was investigated in \cite{Ch2_R44}. A probabilistic user selection scheme is proposed alongside the use of ANNs to estimate unselected users' local FL models, showing a reduction in convergence time and improvement in model accuracy through simulations.

Federated deep learning approaches for cybersecurity in IoT applications are reviewed and experimentally analyzed in \cite{Ch2_R45}. The article highlights the superiority of \ac{fl} in preserving IoT device data privacy and enhancing attack detection accuracy across various deep learning models.

The challenge of training \ac{fl} algorithms over wireless networks is studied in \cite{Ch2_R46}, where the impact of wireless factors on training quality and the optimization of learning, wireless resource allocation, and user selection to minimize FL loss function are discussed. Simulation results demonstrate the effectiveness of the proposed framework in improving identification accuracy.

In \cite{Ch2_R47}, an investigation is conducted into the allocation of energy-efficient transmission and computation resources for \ac{fl} over wireless communication networks. The model considered involves users training local FL models with their data and sending these to a base station (BS), which then aggregates these models and broadcasts the aggregated model back to the users. The optimization problem formulated aims to minimize total energy consumption under a latency constraint, with an iterative algorithm proposed that derives closed-form solutions for various parameters at every step. Initial feasible solutions are generated through a bisection-based algorithm aimed at minimizing completion time. It is shown through numerical results that energy consumption can be reduced by up to 59.5\% compared to conventional FL methods.

In \cite{Ch_1_R9}, a lead federated neuromorphic learning (LFNL) technique is proposed, designed as a decentralized, energy-efficient, brain-inspired computing method utilizing \ac{snn}s. This technique allows edge devices to collaboratively train a global model, preserving privacy and significantly reducing both data traffic and computational latency, with only a slight accuracy loss reported. The efficacy of LFNL in reducing energy consumption while maintaining competitive recognition accuracy, despite uneven dataset distribution, is demonstrated through experimental results.

The aggregation strategies in \ac{fl} through a comprehensive mathematical convergence analysis, leading to the derivation of novel aggregation algorithms that adapt model architecture by differentiating client contributions based on loss values, is presented in \cite{Ch2_R49}. This approach, which extends beyond theoretical assumptions to practical performance evaluations, is compared with the conventional FedAvg method across both IID and non-IID settings without additional hypotheses.

An edge-based backhaul (BH) selection technique aimed at improving traffic delivery by leveraging multiobjective feedback, utilizing advantage-actor-critic deep reinforcement learning methods, is introduced in \cite{Ch2_R50}. To enhance DRL training performance in large-scale IoT system deployments, \ac{fl} is applied, enabling collaboration among multiple edge devices. The effectiveness of this federated DRL approach in solving the BH selection problem is verified through extensive simulations.

In \cite{Ch2_R51}, a hierarchical trajectory planning method named HALOES, which employs deep reinforcement learning within a \ac{fl} scheme, is proposed for efficient, automatic parking in narrow spaces. This method combines high-level deep reinforcement learning with low-level optimization, improving planning time and model generalization capabilities while protecting data privacy during model parameter aggregation. Simulation results affirm HALOES's efficiency in various narrow-space parking scenarios.

The hierarchical \ac{fl} paradigm of HiFL is discussed in \cite{Ch2_R52}, which aims to mitigate communication overhead in FL systems by integrating synchronous client-edge and asynchronous edge-cloud model aggregations. An enhanced design, HiFlash, incorporates adaptive staleness control and a heterogeneity-aware client-edge association strategy through deep reinforcement learning, demonstrating improved system efficiency, communication reduction, and maintained model accuracy in experiments.

A survey on poisoning attacks and defense strategies in \ac{fl} from a privacy-preserving perspective, classifying and analyzing poisoning attacks and defense mechanisms, is provided in \cite{Ch2_R53}. The paper highlights the urgency of defending against poisoning attacks and suggests potential research directions for both attack and defense strategies, emphasizing the need for systematic reviews in this area.