\chapter*{Abstract}

This thesis presents a comprehensive study of multi-agent reinforcement learning using Spiking Neural Networks (SNNs) for decentralized control in edge computing environments. Chapter 1 introduces the motivation and provides a literature review, highlighting the need for biologically inspired neural models to achieve robust and energy-efficient learning in dynamic multi-agent systems. 

Chapter 2 develops adaptive mechanisms for SNNs, focusing on stabilizing learning under sparse conditions for optimal problems and balancing synaptic plasticity with network equilibrium. 

Chapter 3 explores federated learning as a lightweight strategy for sharing matured SNN policies among agents, aiming to reduce communication overhead while maintaining decentralized autonomy. This chapter demonstrates that event-triggered policy exchanges can improve efficiency without requiring full synchronization or centralized orchestration.

Chapter 4 presents the main simulation results, introducing a vision-based multi-agent docking framework that combines Dynamic Vision Sensor (DVS) event streams, adaptive light modulation, and deep SNN controllers. The proposed system demonstrates robust cooperative docking, precise formation control, and resilience to actuation constraints, validating the effectiveness of neuromorphic approaches for complex multi-agent coordination tasks.

