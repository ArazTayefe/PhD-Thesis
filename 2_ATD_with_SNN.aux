\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Ch5_ATD4}
\citation{Ch5_ATD4-5}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Modular Learning in SNNs for Optimal Multi-Agent Decision-Making}{22}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{22}{section.2.1}\protected@file@percent }
\citation{Ch5_ATD6}
\citation{Ch5_ATD6}
\citation{Ch5_ATD7}
\citation{Ch5_STDP2}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The ATD problem and SNN-based solution}{23}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Learning using R-STDP}{23}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Active target defense game with three agents (LOS angles are shown for both agents).}}{24}{figure.2.1}\protected@file@percent }
\newlabel{fig:my_label4}{{2.1}{24}{Active target defense game with three agents (LOS angles are shown for both agents)}{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Two \ac {snn}s simultaneously play and control the invader and defender. Each agent uses the LOS angle and relative velocities for training.}}{24}{figure.2.2}\protected@file@percent }
\newlabel{fig:my_label2}{{2.2}{24}{Two \ac {snn}s simultaneously play and control the invader and defender. Each agent uses the LOS angle and relative velocities for training}{figure.2.2}{}}
\citation{Ch_1_R22}
\newlabel{Eq.14}{{2.1}{25}{Learning using R-STDP}{equation.2.3.1}{}}
\newlabel{Eq.15}{{2.2}{25}{Learning using R-STDP}{equation.2.3.2}{}}
\newlabel{Eq.16}{{2.3}{25}{Learning using R-STDP}{equation.2.3.3}{}}
\citation{Ch5_STDP2}
\newlabel{Eq.17}{{2.4}{26}{Learning using R-STDP}{equation.2.3.4}{}}
\newlabel{Eq.18}{{2.5}{26}{Learning using R-STDP}{equation.2.3.5}{}}
\newlabel{Eq.19}{{2.6}{26}{Learning using R-STDP}{equation.2.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Relative velocities and LOS angles used in the reward function}}{27}{figure.2.3}\protected@file@percent }
\newlabel{fig:my_label5}{{2.3}{27}{Relative velocities and LOS angles used in the reward function}{figure.2.3}{}}
\newlabel{Eq.20}{{2.7}{27}{Learning using R-STDP}{equation.2.3.7}{}}
\newlabel{Eq.21}{{2.8}{27}{Learning using R-STDP}{equation.2.3.8}{}}
\newlabel{Eq.22}{{2.9}{27}{Learning using R-STDP}{equation.2.3.9}{}}
\newlabel{Eq.23}{{2.10}{27}{Learning using R-STDP}{equation.2.3.10}{}}
\citation{Ch5_SNNModel1}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Network structure and encoding method}{28}{section.2.4}\protected@file@percent }
\newlabel{Eq.25}{{2.4}{28}{Network structure and encoding method}{figure.2.6}{}}
\newlabel{Eq.25-1}{{2.11}{28}{Network structure and encoding method}{equation.2.4.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Network structure and encoding process for the input layer (Defender). Each neuron is associated with a membership function in \ac {grf}. The \ac {grf} encodes an input State ($S^t$) at each time step. There is both a training phase when ($\alpha = 0$) and an operating phase when ($\alpha = 1)$.}}{29}{figure.2.4}\protected@file@percent }
\newlabel{fig:my_label1}{{2.4}{29}{Network structure and encoding process for the input layer (Defender). Each neuron is associated with a membership function in \ac {grf}. The \ac {grf} encodes an input State ($S^t$) at each time step. There is both a training phase when ($\alpha = 0$) and an operating phase when ($\alpha = 1)$}{figure.2.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Weight training algorithm}}{30}{algorithm.1}\protected@file@percent }
\newlabel{Algorithm4.1}{{1}{30}{Network structure and encoding method}{algorithm.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces An illustration of input current to \ac {snn} and synaptic current to the output neurons after training for a single connection.}}{31}{figure.2.5}\protected@file@percent }
\newlabel{fig:my_label8}{{2.5}{31}{An illustration of input current to \ac {snn} and synaptic current to the output neurons after training for a single connection}{figure.2.5}{}}
\newlabel{Eq.26}{{2.12}{31}{Network structure and encoding method}{equation.2.4.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Reward, eligibility trace, and weight change during the simulation for $W^{3-3}$. The $C(t) R(t)$ changes the synaptic weight by considering activation strength and reward value.}}{32}{figure.2.6}\protected@file@percent }
\newlabel{fig:my_label9}{{2.6}{32}{Reward, eligibility trace, and weight change during the simulation for $W^{3-3}$. The $C(t) R(t)$ changes the synaptic weight by considering activation strength and reward value}{figure.2.6}{}}
\citation{Ch_1_R10}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Results}{33}{section.2.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Parameter values for LIF neuron model \cite  {Ch_1_R10}}}{33}{table.2.1}\protected@file@percent }
\newlabel{Table4.1}{{2.1}{33}{Parameter values for LIF neuron model \cite {Ch_1_R10}}{table.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Parameter values for \ac {stdp}}}{33}{table.2.2}\protected@file@percent }
\newlabel{Table4.2}{{2.2}{33}{Parameter values for \ac {stdp}}{table.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces \ac {snn}'s performance during training. Hollow circles show the initial positions.}}{34}{figure.2.7}\protected@file@percent }
\newlabel{fig:my_label6}{{2.7}{34}{\ac {snn}'s performance during training. Hollow circles show the initial positions}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Changes in synaptic weights during training. Only Maximum synaptic weights for both agents are shown.}}{35}{figure.2.8}\protected@file@percent }
\newlabel{fig:my_label11}{{2.8}{35}{Changes in synaptic weights during training. Only Maximum synaptic weights for both agents are shown}{figure.2.8}{}}
\citation{Ch5_ATD6}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Simulation without noise}{36}{subsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces \ac {snn}'s performance after training. The highlighted region shows the defender's dominant region. The purple dot is the optimal capture point.}}{36}{figure.2.9}\protected@file@percent }
\newlabel{fig:my_label7}{{2.9}{36}{\ac {snn}'s performance after training. The highlighted region shows the defender's dominant region. The purple dot is the optimal capture point}{figure.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Simulation with noise}{37}{subsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \ac {snn}'s performance in noisy conditions. A white Gaussian noise with a variance of 0.01 is added to the measured inputs.}}{38}{figure.2.10}\protected@file@percent }
\newlabel{fig:my_label12}{{2.10}{38}{\ac {snn}'s performance in noisy conditions. A white Gaussian noise with a variance of 0.01 is added to the measured inputs}{figure.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Conclusion}{38}{section.2.6}\protected@file@percent }
\@setckpt{2_ATD_with_SNN}{
\setcounter{page}{40}
\setcounter{equation}{12}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{2}
\setcounter{Maxaffil}{2}
\setcounter{authors}{0}
\setcounter{affil}{0}
\setcounter{r@tfl@t}{0}
\setcounter{Item}{5}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{17}
\setcounter{float@type}{8}
\setcounter{algorithm}{1}
\setcounter{ALG@line}{35}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{nlinenum}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{49}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{0}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{0}
\setcounter{theorem}{0}
\setcounter{section@level}{1}
}
