\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Ch5_ATD4}
\citation{Ch5_ATD4-5}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Learning a Policy for Pursuit-Evasion Games Using Spiking Neural Networks and the STDP Algorithm}{32}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Ch:ch5}{{5}{32}{Learning a Policy for Pursuit-Evasion Games Using Spiking Neural Networks and the STDP Algorithm}{chapter.5}{}}
\newlabel{sec:proposed}{{5}{32}{Learning a Policy for Pursuit-Evasion Games Using Spiking Neural Networks and the STDP Algorithm}{chapter.5}{}}
\citation{Ch5_ATD6}
\citation{Ch5_ATD6}
\citation{Ch5_ATD7}
\citation{Ch5_STDP2}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}The ATD problem and SNN-based solution}{33}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Active target defense game with three agents (LOS angles are shown for both agents).}}{34}{figure.5.1}\protected@file@percent }
\newlabel{fig:my_label4}{{5.1}{34}{Active target defense game with three agents (LOS angles are shown for both agents)}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Two SNNs simultaneously play and control the invader and defender. Each agent uses the LOS angle and relative velocities for training.}}{34}{figure.5.2}\protected@file@percent }
\newlabel{fig:my_label2}{{5.2}{34}{Two SNNs simultaneously play and control the invader and defender. Each agent uses the LOS angle and relative velocities for training}{figure.5.2}{}}
\citation{Ch5_STDP1}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Learning using STDP}{35}{section.5.2}\protected@file@percent }
\newlabel{Eq.14}{{5.1}{35}{Learning using STDP}{equation.5.2.1}{}}
\newlabel{Eq.15}{{5.2}{35}{Learning using STDP}{equation.5.2.2}{}}
\newlabel{Eq.16}{{5.3}{35}{Learning using STDP}{equation.5.2.3}{}}
\citation{Ch5_STDP2}
\newlabel{Eq.17}{{5.4}{36}{Learning using STDP}{equation.5.2.4}{}}
\newlabel{Eq.18}{{5.5}{36}{Learning using STDP}{equation.5.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Relative velocities and LOS angles used in the reward function}}{37}{figure.5.3}\protected@file@percent }
\newlabel{fig:my_label5}{{5.3}{37}{Relative velocities and LOS angles used in the reward function}{figure.5.3}{}}
\newlabel{Eq.19}{{5.6}{37}{Learning using STDP}{equation.5.2.6}{}}
\newlabel{Eq.20}{{5.7}{37}{Learning using STDP}{equation.5.2.7}{}}
\newlabel{Eq.21}{{5.8}{37}{Learning using STDP}{equation.5.2.8}{}}
\newlabel{Eq.22}{{5.9}{37}{Learning using STDP}{equation.5.2.9}{}}
\citation{Ch5_SNNModel1}
\newlabel{Eq.23}{{5.10}{38}{Learning using STDP}{equation.5.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Network structure and encoding method}{38}{section.5.3}\protected@file@percent }
\newlabel{Eq.25}{{5.3}{38}{Network structure and encoding method}{figure.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Network structure and encoding process for the input layer (Defender). Each neuron is associated with a membership function in GRF. The GRF encodes an input State ($S^t$) at each time step. There is both a training phase when ($\alpha = 0$) and an operating phase when ($\alpha = 1)$.}}{39}{figure.5.4}\protected@file@percent }
\newlabel{fig:my_label1}{{5.4}{39}{Network structure and encoding process for the input layer (Defender). Each neuron is associated with a membership function in GRF. The GRF encodes an input State ($S^t$) at each time step. There is both a training phase when ($\alpha = 0$) and an operating phase when ($\alpha = 1)$}{figure.5.4}{}}
\newlabel{Eq.25-1}{{5.11}{40}{Network structure and encoding method}{equation.5.3.11}{}}
\newlabel{Eq.26}{{5.12}{40}{Network structure and encoding method}{equation.5.3.12}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Weight training algorithm}}{41}{algorithm.2}\protected@file@percent }
\newlabel{alg1}{{2}{41}{Network structure and encoding method}{algorithm.2}{}}
\citation{Ch5_R1}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Parameter values for LIF neuron model \cite  {Ch5_R1}}}{42}{table.5.1}\protected@file@percent }
\newlabel{tab1}{{5.1}{42}{Parameter values for LIF neuron model \cite {Ch5_R1}}{table.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Results}{42}{section.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Parameter values for STDP}}{43}{table.5.2}\protected@file@percent }
\newlabel{tab2}{{5.2}{43}{Parameter values for STDP}{table.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces SNN's performance during training. Hollow circles show the initial positions.}}{43}{figure.5.5}\protected@file@percent }
\newlabel{fig:my_label6}{{5.5}{43}{SNN's performance during training. Hollow circles show the initial positions}{figure.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Changes in synaptic weights during training. Only Maximum synaptic weights for both agents are shown.}}{44}{figure.5.6}\protected@file@percent }
\newlabel{fig:my_label11}{{5.6}{44}{Changes in synaptic weights during training. Only Maximum synaptic weights for both agents are shown}{figure.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces An illustration of input current to SNN and synaptic current to the output neurons after training for a single connection.}}{46}{figure.5.7}\protected@file@percent }
\newlabel{fig:my_label8}{{5.7}{46}{An illustration of input current to SNN and synaptic current to the output neurons after training for a single connection}{figure.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Reward, eligibility trace, and weight change during the simulation for $W^{3-3}$. The $C(t) R(t)$ changes the synaptic weight by considering activation strength and reward value.}}{46}{figure.5.8}\protected@file@percent }
\newlabel{fig:my_label9}{{5.8}{46}{Reward, eligibility trace, and weight change during the simulation for $W^{3-3}$. The $C(t) R(t)$ changes the synaptic weight by considering activation strength and reward value}{figure.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces SNN's performance after training. The highlighted region shows the defender's dominant region. The purple dot is the optimal capture point.}}{47}{figure.5.9}\protected@file@percent }
\newlabel{fig:my_label7}{{5.9}{47}{SNN's performance after training. The highlighted region shows the defender's dominant region. The purple dot is the optimal capture point}{figure.5.9}{}}
\citation{Ch5_ATD6}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Simulation without noise}{48}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Simulation with noise}{48}{subsection.5.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces SNN's performance in noisy conditions. A white Gaussian noise with a variance of 0.01 is added to the measured inputs.}}{49}{figure.5.10}\protected@file@percent }
\newlabel{fig:my_label12}{{5.10}{49}{SNN's performance in noisy conditions. A white Gaussian noise with a variance of 0.01 is added to the measured inputs}{figure.5.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusion}{50}{section.5.5}\protected@file@percent }
\@setckpt{chapter_5}{
\setcounter{page}{51}
\setcounter{equation}{12}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{2}
\setcounter{Maxaffil}{2}
\setcounter{authors}{0}
\setcounter{affil}{0}
\setcounter{r@tfl@t}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALG@line}{35}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{nlinenum}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{22}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{58}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{0}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{0}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{section@level}{1}
}
