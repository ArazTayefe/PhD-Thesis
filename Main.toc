\acswitchoff 
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Spiking Neural Networks: Models and Learning Algorithms}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Neuron model}{1}{subsection.1.1.1}%
\contentsline {subsubsection}{Leaky-Integrate and Fire (LIF) model}{1}{section*.6}%
\contentsline {subsubsection}{Hodgkin-Huxley Model}{3}{section*.7}%
\contentsline {subsubsection}{FitzHugh-Nagumo Model}{4}{section*.8}%
\contentsline {subsubsection}{Izhikevich Model}{4}{section*.9}%
\contentsline {subsection}{\numberline {1.1.2}Training Algorithms}{5}{subsection.1.1.2}%
\contentsline {subsubsection}{Reward-modulated Spike-Timing-Dependent Plasticity (R-STDP)}{5}{section*.10}%
\contentsline {subsubsection}{Temporal Difference Learning (TD-Learning)}{5}{section*.11}%
\contentsline {subsubsection}{Deep Q-Learning for Spiking Neural Networks (DQSNN)}{6}{section*.12}%
\contentsline {subsubsection}{Policy Gradient Methods for SNNs}{6}{section*.13}%
\contentsline {subsubsection}{Spiking Actor-Critic (SAC)}{6}{section*.14}%
\contentsline {subsubsection}{Spiking Proximal Policy Optimization (SPPO)}{7}{section*.15}%
\contentsline {subsection}{\numberline {1.1.3}Discussion}{7}{subsection.1.1.3}%
\contentsline {section}{\numberline {1.2}Federated Learning for SNNs: Consensus Flying Scenario}{8}{section.1.2}%
\contentsline {chapter}{\numberline {2}Literature review}{9}{chapter.2}%
\contentsline {chapter}{\numberline {3}Modular Learning in SNNs for Optimal Multi-Agent Decision-Making}{18}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{18}{section.3.1}%
\contentsline {section}{\numberline {3.2}The ATD problem and SNN-based solution}{19}{section.3.2}%
\contentsline {section}{\numberline {3.3}Learning using R-STDP}{20}{section.3.3}%
\contentsline {section}{\numberline {3.4}Network structure and encoding method}{24}{section.3.4}%
\contentsline {section}{\numberline {3.5}Results}{28}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Simulation without noise}{31}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Simulation with noise}{34}{subsection.3.5.2}%
\contentsline {section}{\numberline {3.6}Conclusion}{36}{section.3.6}%
\contentsline {chapter}{\numberline {4}Integration of R-STDP and Federated Learning}{37}{chapter.4}%
\contentsline {section}{\numberline {4.1}Consensus Flying Problem}{37}{section.4.1}%
\contentsline {section}{\numberline {4.2}Proposed Method}{39}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Network Structure}{39}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Training algorithm}{44}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Weight Stabilization using Reward-Modulated Competitive Synaptic Equilibrium (R-CSE)}{45}{subsection.4.2.3}%
\contentsline {subsection}{\numberline {4.2.4}Federated Learning for Consensus Flying}{48}{subsection.4.2.4}%
\contentsline {section}{\numberline {4.3}Results and Discussion}{52}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Simulation without \ac {fl}}{52}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Simulation with \ac {fl} and \ac {RCSE}}{62}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Conclusion}{65}{section.4.4}%
\contentsline {chapter}{\numberline {5}Proposed Method}{67}{chapter.5}%
