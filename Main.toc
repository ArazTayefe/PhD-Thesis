\acswitchoff 
\contentsline {chapter}{\numberline {1}Spiking Neural Networks: Models and Learning Algorithms}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Multi-agent cooperation over cellular network}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Neuron model}{1}{section.1.2}%
\contentsline {chapter}{\numberline {2}Literature review}{4}{chapter.2}%
\contentsline {chapter}{\numberline {3}Control and navigation of the leader drone over 5G network}{13}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{13}{section.3.1}%
\contentsline {section}{\numberline {3.2}Quadrotor dynamic model}{15}{section.3.2}%
\contentsline {section}{\numberline {3.3}Linear PD Controller}{16}{section.3.3}%
\contentsline {section}{\numberline {3.4}Backstepping Controller}{18}{section.3.4}%
\contentsline {section}{\numberline {3.5}State Estimator}{20}{section.3.5}%
\contentsline {section}{\numberline {3.6}Delay Estimator}{22}{section.3.6}%
\contentsline {section}{\numberline {3.7}Results}{23}{section.3.7}%
\contentsline {paragraph}{Exact knowledge of delay}{25}{section*.5}%
\contentsline {paragraph}{Delay estimation based on the Markov model}{26}{section*.6}%
\contentsline {paragraph}{Change in delay values and distributions}{30}{section*.7}%
\contentsline {section}{\numberline {3.8}Experimental setup}{33}{section.3.8}%
\contentsline {section}{\numberline {3.9}Conclusion}{33}{section.3.9}%
\contentsline {chapter}{\numberline {4}Modular Learning in SNNs for Optimal Multi-Agent Decision-Making}{35}{chapter.4}%
\contentsline {section}{\numberline {4.1}The ATD problem and SNN-based solution}{36}{section.4.1}%
\contentsline {section}{\numberline {4.2}Learning using R-STDP}{38}{section.4.2}%
\contentsline {section}{\numberline {4.3}Network structure and encoding method}{41}{section.4.3}%
\contentsline {section}{\numberline {4.4}Results}{45}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Simulation without noise}{48}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Simulation with noise}{51}{subsection.4.4.2}%
\contentsline {section}{\numberline {4.5}Conclusion}{53}{section.4.5}%
\contentsline {chapter}{\numberline {5}Enhancing Cooperative Multi-agent Reinforcement Learning through the Integration of R-STDP and Federated Learning}{54}{chapter.5}%
\contentsline {section}{\numberline {5.1}Consensus Flying Problem}{54}{section.5.1}%
\contentsline {section}{\numberline {5.2}Proposed Method}{56}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Network Structure}{56}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Training algorithm}{60}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Weight Stabilization using Reward-Modulated Competitive Synaptic Equilibrium (R-CSE)}{62}{subsection.5.2.3}%
\contentsline {subsection}{\numberline {5.2.4}Federated Learning for Consensus Flying}{65}{subsection.5.2.4}%
\contentsline {section}{\numberline {5.3}Results and Discussion}{70}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Simulation without FL}{70}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Simulation with FL and R-CSE}{79}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4}Conclusion}{83}{section.5.4}%
\contentsline {chapter}{\numberline {6}Proposed Method}{84}{chapter.6}%
