\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {chapter}{\numberline {2}Literature review}{2}{chapter.2}%
\contentsline {chapter}{\numberline {3}Spiking Neural Networks: Models and Learning Algorithms}{4}{chapter.3}%
\contentsline {section}{\numberline {3.1}Neuron model}{4}{section.3.1}%
\contentsline {chapter}{\numberline {4}Comparison of Cellular Network Controllers for Quadrotors Experiencing Time Delay}{7}{chapter.4}%
\contentsline {section}{\numberline {4.1}Proposed method}{7}{section.4.1}%
\contentsline {section}{\numberline {4.2}Quadrotor dynamic model}{11}{section.4.2}%
\contentsline {section}{\numberline {4.3}Controller Design}{13}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Linear PD Controller}{13}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Backstepping Controller}{14}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}State Estimator}{16}{section.4.4}%
\contentsline {section}{\numberline {4.5}Delay Estimator}{18}{section.4.5}%
\contentsline {section}{\numberline {4.6}Results}{19}{section.4.6}%
\contentsline {paragraph}{Exact knowledge of delay}{21}{section*.5}%
\contentsline {paragraph}{Delay estimation based on the Markov model}{21}{section*.6}%
\contentsline {paragraph}{Change in delay values and distributions}{28}{section*.7}%
\contentsline {section}{\numberline {4.7}Conclusion}{30}{section.4.7}%
\contentsline {chapter}{\numberline {5}Learning a Policy for Pursuit-Evasion Games Using Spiking Neural Networks and the STDP Algorithm}{32}{chapter.5}%
\contentsline {section}{\numberline {5.1}The ATD problem and SNN-based solution}{33}{section.5.1}%
\contentsline {section}{\numberline {5.2}Learning using STDP}{35}{section.5.2}%
\contentsline {section}{\numberline {5.3}Network structure and encoding method}{38}{section.5.3}%
\contentsline {section}{\numberline {5.4}Results}{42}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Simulation without noise}{48}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Simulation with noise}{48}{subsection.5.4.2}%
\contentsline {section}{\numberline {5.5}Conclusion}{50}{section.5.5}%
\contentsline {chapter}{\numberline {6}Enhancing Cooperative Multi-agent Reinforcement Learning through the Integration of R-STDP and Federated Learning}{51}{chapter.6}%
\contentsline {section}{\numberline {6.1}Preliminaries}{51}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Consensus Flying Problem}{51}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}Neuron Model}{52}{subsection.6.1.2}%
\contentsline {section}{\numberline {6.2}Proposed Method}{54}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Network Structure}{54}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Training algorithm}{59}{subsection.6.2.2}%
\contentsline {subsection}{\numberline {6.2.3}Weight Stabilization using Reward-Modulated Competitive Synaptic Equilibrium (R-CSE)}{61}{subsection.6.2.3}%
\contentsline {subsection}{\numberline {6.2.4}Federated Learning for Consensus Flying}{64}{subsection.6.2.4}%
\contentsline {section}{\numberline {6.3}Results and Discussion}{68}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Simulation without \gls {fl}}{68}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Simulation with \gls {fl} and \gls {RCSE}}{77}{subsection.6.3.2}%
\contentsline {section}{\numberline {6.4}Conclusion}{80}{section.6.4}%
