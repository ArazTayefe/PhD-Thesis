The synchronization and coordination of multi-agent systems (MAS) are central to formation tasks in space docking missions, with leader-following consensus often achieved using event-triggered impulsive control for fast agreement without continuous communication \cite{tan2019consensus}, or dynamic event-triggered mechanisms with auxiliary variables to prevent Zeno behavior, where an infinite number of events are triggered within a finite time interval, and reduce bandwidth usage in spacecraft clusters \cite{du2020dynamic}. Fixed-time consensus schemes ensure convergence within a specified time, supporting tightly synchronized docking maneuvers \cite{liu2020fixedtime}. Formation control under strict timing and resource constraints has progressed through neural-network-based and adaptive methods, including event-based adaptive neural network controllers using radial basis function networks to handle nonlinear uncertainties \cite{cao2024event} and adaptive tracking controls that integrate disturbance observers and event-triggered thresholds for maintaining performance with minimal updates \cite{zhang2020eventtriggered}. Deep learning has further enhanced MAS control, with centralized training and decentralized execution in deep reinforcement learning proving effective for complex coordination tasks such as distributed assembly and formation \cite{nguyen2020deep}. In contrast, event-triggered adaptive control for nonaffine dynamics and fully distributed NN-based adaptive control enable reduced communication and asymptotic synchronization in heterogeneous spacecraft formations \cite{liang2021neural, shen2020neural}. Robustness to sensor faults, input saturation, and precise timing remains essential in docking, which is achieved through event-triggered controllers using Nussbaum functions and barrier Lyapunov methods \cite{liu2021event}, alongside prescribed-time consensus frameworks that enable strict temporal adherence in docking sequences \cite{wang2019prescribed}.

Autonomous docking has been extensively studied across marine, aerial, and space domains. Vision-based pose estimation techniques have first demonstrated the real-world docking of work-class Remotely Operated Vehicles (ROVs) to both static and dynamic transportation management system platforms, using asymmetrically arranged LED beacons and multi-step image processing integrated with PID control loops \cite{Trslic2020}. Monocular vision systems with novel attitude estimators and particle‐filter trackers have similarly enabled high‐accuracy docking of hovering autonomous underwater vehicles, maintaining continuous marker visibility even under partial occlusion \cite{Figueiredo2020}. These approaches highlight the potential of lightweight vision sensors for proximity operations where acoustic methods may be limited.

In spacecraft applications, adaptive controllers enforcing time‐varying state constraints have been shown to guarantee safe Payload‐carrying docking despite unknown mass variations, by employing Barrier Lyapunov Functions to bound position and velocity errors within decaying envelopes \cite{Sankaranarayanan2025}. Hardware tests on planar air-bearing platforms confirmed that such adaptive schemes outperform unconstrained methods under significant disturbances, a critical requirement for on-orbit servicing missions, where safety is paramount.

Geometric and potential‐based guidance laws have enabled cooperative docking between aerial vehicles. Quadrotor pairs employed artificial potential functions for approach guidance and nonlinear 3D controllers for precise port alignment during docking, demonstrating effective mutual maneuvering in simulation \cite{Nikhilraj2025}. Similarly, VTOL UAVs docking to mobile manipulators achieved on-site recharging through a hybrid visual-servoing and path planning framework, fusing predictive and reactive control to maintain a line of sight and ensure a secure attachment \cite{Narvaez2021}.

Formation control frameworks extended docking capabilities to heterogeneous mobile robot teams. A subsumption architecture combined layered kinematic objectives with robust dynamic controllers to manage autonomous docking, formation switching, and collision avoidance under model uncertainties, verified in simulation with up to a dozen robots \cite{Lashkari2020}. Compact self-assembling modules with parallel-gripper docking and onboard visual perception further illustrate scalable docking and reconfiguration for modular robot chains traversing complex terrains \cite{Li2022}.

In structured environments, sensor-fusion and learning-based methods have improved docking precision. YOLOv7-based detection, fused with LiDAR alignment, enabled robust industrial mobile robot recharging, achieving real-time recognition and sub-decimeter positioning in manufacturing settings \cite{Jia2023}. Regression-based monocular systems trained on LiDAR ground truth can influence ArUco marker distortions to estimate distance and orientation with centimeter-level accuracy, vastly outperforming SolvePnP methods while using only low-cost cameras \cite{Oh2025}.

While these model-based and vision-driven controllers provide strong performance in defined scenarios, proximity missions in space docking, characterized by time-varying dynamics, unforeseen disturbances, and energy constraints, require more adaptive, data-driven control architectures. \ac{snn}s trained via the Reinforcement Learning (RL) paradigm can learn end‐to‐end docking policies from sensory inputs, offering fast inference, on‐chip adaptability, and robustness to unmodeled effects \cite{tang2021deep}.

% Deep SNN

RL paradigms in \ac{snn}s enable agents to learn control policies through trial-and-error interactions, a capability crucial for autonomous proximity operations in space docking. Bio-plausible learning rules, such as Spike-Timing-Dependent Plasticity (STDP) combined with reward signals, have been shown to approximate backpropagation and support multi-layer learning \cite{TavanaeiMaida2018BPSTDP, AnwaniRajendran2020NormAD, TaherkhaniBelatreche2018BPSL}. By incorporating eligibility traces and global reward feedback, these methods allow \ac{snn}s to adjust synaptic weights in response to docking success metrics, facilitating rapid adaptation to new docking trajectories.

Scaling RL-trained \ac{snn} architectures to deep networks requires addressing vanishing spike activity and ensuring information propagation across layers. Techniques such as Spike Activation Lift Training (SALT) and Switched-BN enhance spike throughput in very deep \ac{snn}s, improving learning capacity under RL-like reward cues \cite{KimPanda2021SALT}. Likewise, weighted spikes encode multi-bit information per event, reducing decision latency, an essential factor for timely thruster commands during docking \cite{KimHuh2018Weighted}. These advances pave the way for deep \ac{snn}s that learn complex docking maneuvers through RL while maintaining low energy consumption.

Proximity missions impose strict hardware constraints on power, area, and latency. Neuromorphic signal‐processing systems that employ ternary spike encoding and quantized \ac{snn}s achieve substantial memory and energy savings, making on-board RL plausible \cite{WangZhang2025Ternary, LinYuan2020ReRAM}. In-memory ReRAM-based architectures with ternary deep \ac{snn}s further reduce data movement and support fast synaptic updates under RL protocols \cite{LinYuan2020ReRAM}. Such hardware platforms can host RL-trained \ac{snn} controllers directly on spacecraft avionics, offering robust, low-power autonomy.

Accurate temporal and spatial precision is vital for docking proximity control. Fractional‐order STDP (FO‐STDP) enhances gradient descent in \ac{snn}s, improving convergence speed and accuracy in gesture‐like input scenarios analogous to docking sensor streams \cite{YangVoyles2025FOSTDGD}. Biologically plausible multi‐spike learning algorithms integrate synaptic delay training to fine‐tune event timing, enabling precise control of approach velocities and alignment \cite{TaherkhaniBelatreche2018BPSL}.

Optimizing the training pipeline further accelerates RL convergence in \ac{snn}s. Parameter initialization based on neuronal response asymptotes prevents vanishing gradients, enabling deeper RL‐trained \ac{snn}s to learn docking policies from sparse reward signals \cite{DingZhang2025Init}. Modeling ANN‐\ac{snn} conversion residuals as additive noise yields low‐latency networks that maintain policy fidelity under time constraints \cite{HuangDing2025Residual}. Hybrid training methods that jointly optimize membrane leak, thresholds, and weights reduce inference timesteps to a few ms\cite{XuLi2021DIET}.

These advances in RL‐trained \ac{snn}s, spanning learning rules, deep architectures, neuromorphic hardware, temporal precision, and training optimizations, provide a unified foundation for developing energy‐efficient, low‐latency controllers for space docking proximity missions. \ac{dvs} cameras can supply sparse, event‐driven input streams to these \ac{snn} controllers, further enhancing their responsiveness and energy efficiency in docking scenarios.

% DVS 

\ac{dvs}s offer a fundamentally different paradigm from conventional frame-based cameras by asynchronously encoding pixel-level brightness changes as a sparse stream of events, yielding microsecond-scale temporal resolution and minimal data redundancy. This event-driven operation allows \ac{dvs} to detect fast motions with latencies as low as 3 milliseconds, while consuming only a fraction of the power needed by CMOS imagers \cite{Lee2014RealTimeGesture,Seifozzakerini2018Hough}. The high dynamic range (up to 120 dB) of \ac{dvs} further enables reliable operation under extreme illumination variations without saturating the sensor \cite{Seifozzakerini2018Hough,Bichler2012Extraction}. These attributes are particularly advantageous for proximity operations in space docking, where rapid, energy-efficient sensing of relative motion is critical.

\ac{snn}s naturally complement \ac{dvs} by processing event streams natively through time-encoded spikes, enabling highly efficient feature extraction and pattern recognition. Early work demonstrated that the STDP can robustly learn temporal correlations from raw \ac{dvs} data, extracting motion patterns such as vehicle trajectories with minimal synaptic complexity and high noise tolerance \cite{Bichler2012Extraction,Thiele2018EventBased}. Architectures with localized lateral inhibition have been shown to learn hierarchical features in a single pass, achieving an accuracy above 96\,\% with only one presentation of the data \cite{Thiele2018EventBased,Zhao2014Feedforward}.

Recent advances have expanded \ac{dvs}–\ac{snn} systems to more complex applications relevant to autonomous navigation and control. For instance, deep convolutional \ac{snn}s trained with surrogate gradients achieve competitive object localization accuracy with up to 126× energy savings compared to ANNs \cite{Barchid2023Localization,Zanatta2023Directly}. In RL scenarios, directly-trained \ac{snn}s on neuromorphic accelerators have shown superior generalization for UAV obstacle avoidance, reaching goals 98\,\% of the time while consuming an order of magnitude less energy than CNN counterparts \cite{Zanatta2023Directly,Salt2019Parameter}. Event-based action recognition using novel benchmarks, such as \ac{dvs}-Gesture-Chain, highlights the inherent temporal processing advantages of \ac{snn}s, enabling feed-forward networks to perceive event sequences without recurrent connections \cite{VicenteSola2024SpikingAction,Zanatta2023Directly}.

For space docking missions, precise detection of geometric features such as planar edges and circular apertures is essential for alignment and approach control. Event-based adaptations of the Hough Transform implemented in \ac{snn}s allow real-time line detection and 3D parameter estimation directly on \ac{dvs} outputs with no external memory or CPU, significantly reducing latency and power consumption \cite{Seifozzakerini2018Hough}. Self-supervised information bottleneck learning further enhances event-based optical flow estimation, improving robustness to noise and reducing energy usage by over 90\,\% compared to ANN methods \cite{Yang2024SelfSupervised}. These capabilities suggest that \ac{dvs}–\ac{snn} pipelines can provide the low-latency, low-power sensing and processing required for autonomous proximity operations in on-orbit docking scenarios.