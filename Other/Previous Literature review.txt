% Spiking Neural Networks (SNNs) are the third generation of neural networks that have gained attention for their lower power consumption due to their event-driven nature and sparse representation of data, which is based on real biological neurons \cite{Ch5_S0, Ch5_S0_1, Ch5_S0_2}. The SNNs work based on spikes. Since the spikes are not continuous, only a part of the network is active at a given time. Also, successful hardware implementation of SNNs on neuromorphic chips like TrueNorth \cite{Ch5_S1} and Loihi \cite{Ch5_S2} makes them good candidates for robotic applications.

% Studies have shown that the SNN can learn particular tasks autonomously to control robotic platforms. In \cite{Ch5_S3}, a Reward Modulated Spike-Timing-Dependent Plasticity (R-STDP) algorithm is used to control a mobile robot. Different States of the mobile robot (e.g., acceleration and deceleration) are defined as different frequencies for the input sensory layer. The SNN learns to navigate autonomously in the environment while avoiding obstacles.

% There are several successful implementations of SNNs in learning nonlinear dynamics. In \cite{Ch5_S4}, a chaotic reservoir of spiking neurons is trained to control a robotic arm adaptively. The network is able to follow the desired trajectory with few neurons. SNN's capability in controlling a robotic arm with three degrees of freedom is also studied in \cite{Ch5_S5}, where the SNN has 218 neurons. The results are compared with real behavioural and neural data. The results showed that the SNN can adapt the Jacobian matrix when dynamic properties are changed over time.

% In \cite{Ch5_S7}, an SNN is trained using the R-STDP algorithm and lateral inhibition mechanism to control a mobile robot in an environment with an obstacle. The lateral inhibition helps SNNs switch between target tracking and obstacle avoidance tasks during and after learning. The results showed that the SNNs have the potential to take control in complex scenarios. 


% In \cite{Ch5_ATD0}, a method based on the reinforcement learning algorithm is proposed to help an invader minimize its distance from a territory before being captured. In \cite{Ch5_ATD1}, an interceptor is used to defend a target from an attacking intruder. This type of differential game is addressed as Defending an Asset, which is solved using Linear Quadratic formulation. In \cite{Ch5_ATD2}, the problem is solved for multi-player cases but split into two simple two-player games.

% Multiple attackers-defenders problems with a stationary target are studied in \cite{Ch5_ATD3}. However, a simple single attacker-single defender case is solved because of the complexity of the numerical solution. Several studies have tried to propose optimal solutions for the ATD problem. A set of optimal solutions are proposed in \cite{Ch5_ATD5} considering reduced State space conditions. Using the geometric method, the attacker played an optimal strategy, capturing the target while maximizing its distance from the defender.



% The consensus problem in flying multi-agent systems, often termed the ``flocking" or ``swarming" challenge, is a critical area of research in aerial robotics. It deals with coordinating and controlling multiple flying agents, ensuring they can work cooperatively without collisions and align towards a common objective. Achieving this consensus is important for different applications, from coordinated surveillance to communication, logistics, and infrastructure monitoring.

% \ac{fl} is an emerging paradigm in machine learning that allows for decentralized training of models across multiple agents without centralizing data~\cite{Ch_1_R9}. Using this approach, robots can benefit from shared experiences while preserving data privacy and reducing communication overheads~\cite{Int2, Int3}.

% The challenge of optimizing communication efficiency in \ac{fl} is addressed in~\cite{Int3}, where a hierarchical approach is introduced, leveraging adaptive staleness control. This method emphasizes that both system-level and data-level heterogeneity in \ac{fl} must be considered. The complexities of wireless network constraints in \ac{fl} are explored in another study, where the focus is placed on optimizing the convergence time. This research indicates that the wireless environment introduces a layer of complexity to the \ac{fl} framework~\cite{Int25}.

% The practical applicability of \ac{fl} in real-world scenarios is demonstrated in~\cite{Int28}, which focuses on hierarchical trajectory planning for narrow-space automated parking. This study highlights the vast potential of \ac{fl} in automation and robotics. An interesting direction in which neuromorphic learning and \ac{fl} converge is presented in ~\cite{Int29}, suggesting future trends where bio-inspired computing and \ac{fl} might merge. A holistic view of the advancements, challenges, and future directions in \ac{fl} is provided in a comprehensive survey, making it clear that \ac{fl} is poised to reshape the landscape of machine learning~\cite{Int30}.

% The aforementioned applications demand real-time decision-making and adaptability to dynamic environments. The Reinforcement Learning (RL) algorithms offer a promising approach to address these challenges by enabling agents to learn optimal policies through interactions with their environment, enabling them to adapt to dynamic scenarios and uncertainties. Specifically, they allow agents to autonomously learn from their experiences, making them well-suited for tasks that require decentralized decision-making and adaptability to unforeseen situations~\cite{Int5}. 

% In addition to RL, the incorporation of \ac{fl} offers a decentralized approach, further advancing learning and adaptability in dynamic and distributed environments. The integration of \ac{fl} with RL presents opportunities for drones to learn and update their policies in a distributed manner collaboratively, leveraging the collective intelligence of the swarm~\cite{Int4}. The advent of the \ac{fl} offers a decentralized training paradigm, allowing agents to learn collaboratively while keeping their data localized. 

% The opportunity and challenge presented by IoT devices in \ac{fl} are highlighted in~\cite{Int4}, where edge computing and deep reinforcement learning are combined for traffic management in IoT. The evolving nature of \ac{fl} applications, branching out from traditional use cases, is demonstrated by this work. Regarding the aggregation methods in \ac{fl}, robust algorithms that provide guarantees against adversarial attacks are introduced, underscoring the security aspect of \ac{fl}~\cite{Int27}.

% By integrating RL with \ac{fl}, researchers envision a new era where flying agents learn and adapt in real-time, making consensus problems more manageable. Fusing these advanced learning techniques can revolutionize consensus mechanisms in flying multi-agent systems. The application of these integrated learning methods in multi-agent systems presents various challenges and opportunities, particularly in formation control and security aspects. 

% Formation control in multi-agent systems, particularly in cost-constrained environments, presents inherent challenges, including managing limited resources, communication constraints, and ensuring scalability and robustness. To address this, research has proposed a Distributed Deformable Configuration Control (DDCC) for multi-robot systems, allowing adaptive shape maintenance among environmental changes. This method leverages local sensing and communication, demonstrating robustness even on low-cost platforms~\cite{Int32}. In contrast, the security dimension of multi-agent systems has been explored in the context of cyber threats. 

% A study presented an edge-based adaptive mechanism to achieve secure consensus in nonlinear multi-agent systems, especially when communication links face threats. This methodology adaptively tunes communication link weights and has exhibited resilience against adversarial conditions, with its stability validated using Lyapunov stability theory~\cite{Int33}. Beyond these security and control aspects, multi-agent systems research also explores hierarchical structures, such as leader-follower dynamics, which are essential in certain applications like maritime operations. 

% Leader-follower consensus mechanisms play a pivotal role in tasks demanding hierarchical structures. Recent work in this domain has introduced finite-time control protocols aiming for rapid leader-follower consensus. Through a specific leader-follower model combined with Lyapunov function techniques, these methods assure stability and convergence within set time frames~\cite{Int34}. Maritime applications, especially concerning the flocking control of Unmanned Surface Vehicles (USVs), have introduced a path-guided model-free flocking control strategy. This approach, devoid of reliance on precise mathematical models, employs Concurrent Learning Extended State Observers (CLESOs) for accurate system state and dynamic estimations, ensuring robust control among maritime uncertainties~\cite{Int35}. In the realm of decision-making within these systems, innovative methodologies like entropy-based consensus are emerging, offering new ways to enhance cooperation and efficiency. 

% A novel methodology in artificial swarms emphasizes entropy-based local negotiation and preference updating for consensus decision-making. This approach, grounded in swarm intelligence, employs an entropy-based metric to gauge consensus levels among agents, informing their negotiation choices. The method has shown promise with faster convergence, higher success rates, and remarkable scalability, making it suitable for diverse applications~\cite{Int31}. Efficient decision-making in multi-agent systems can also be optimized through event-triggering mechanisms, which streamline communication and computation processes. 

% According to recent research, event-triggering mechanisms in multi-agent systems can improve communication and computation. This approach operates with predetermined communication patterns, reducing messages to save resources while ensuring stability and convergence~\cite{Int36}. While optimizing communication in multi-agent systems, emerging methods like \ac{snn} offer groundbreaking approaches to robotic learning and control. 

% \ac{snn}s have gained popularity in robotics due to their ability to replicate the structure and functioning of networks. One crucial characteristic of \ac{snn}s is their capacity to encode and handle information through spikes, which has been demonstrated as a resource energy-saving approach~\cite{Int7, Int8}. The versatility of \ac{snn}s is further expanded through their ability to learn and adapt, which is particularly beneficial in complex tasks like movement planning and nonlinear system control. 

% One of the remarkable features of \ac{snn}s is their ability to learn and adapt. Here, \ac{stdp} plays a key role. \ac{stdp} is a bio-inspired learning rule based on the relative timing of pre- and post-synaptic spikes. Several studies have elucidated the constraints on Hebbian and \ac{stdp} learned weights in spiking neurons, revealing the underlying mechanisms that make this learning paradigm so effective~\cite{Int11}. Furthermore, research has demonstrated that networks trained with local rules, such as \ac{stdp}, can exhibit continuous learning, showcasing their potential in lifelong learning scenarios~\cite{Int12}. To fully utilize the potential of \ac{snn}s, the development of hybrid models and the exploration of various training methodologies are crucial in enhancing their adaptability and efficiency. 

% In robotics, the application of \ac{snn}s has shown promise in various tasks, including movement planning within confined operational spaces. Such applications leverage the temporal dynamics of spiking neurons to achieve collision-free motion, demonstrating the capability of \ac{snn}s to handle complex spatial-temporal challenges~\cite{Int9}. Beyond movement planning, \ac{snn}s have also been employed for nonlinear systems control, providing a robust and adaptive control mechanism~\cite{Int10}. 

% The evolution of \ac{snn} has seen the emergence of models that aim to blend the advantages of machine-inspired approaches. These hybrid systems have proven to enhance the adaptability of \ac{snn}s, making them more suitable for environments encountered in robotics~\cite{Ch_1_R24}. The advancements in classification capabilities offered by integrate and fire models have also expanded the range of \ac{snn} applications in robotics~\cite{Int14}. Notably, Deep Spiking Q Networks, which are trained directly, have shown performance in tasks highlighting the potential of integrating deep learning techniques with \ac{snn}s to achieve human-level control~\cite{Int15}. Exploring direct and indirect training methodologies for \ac{snn}s is vital, particularly in applications such as autonomous vehicle control, where end-to-end learning is essential. 

% Direct and indirect training methodologies for \ac{snn}s have been extensively explored for achieving end-to-end control of vehicles in tasks like lane keeping~\cite{Int17}. Furthermore, incorporating meta neurons into \ac{snn}s has further improved their effectiveness in spatial learning tasks~\cite{Int18}. The adaptability of \ac{snn}s, enhanced through reinforcement and evolutionary learning, opens new avenues in robotics, particularly motor control and changing operational needs. The exploration of learning dynamics in neural networks, particularly in developing neural units that can learn rules and robot dynamics, represents a significant advancement in the capabilities of \ac{snn}s. 

% In line with research on learning dynamics, there is a growing interest in developing networks consisting of neural units. When combined together, these units possess the ability to learn both learning rules and spiking dynamics, thereby enhancing the capabilities exhibited by \ac{snn}~\cite{Int19}.

% The application of reinforcement and evolutionary learning to train \ac{snn}s for motor control has opened new avenues in the field of robotics. Such training methods use \ac{snn}s' adaptability for the changing needs of robots~\cite{Int20, Int21, Ch_1_R5, Int23}. The integration of \ac{fl} with \ac{snn}s presents the synergy between decentralized training strategies and advanced neural network architectures, offering collaborative and privacy-preserving learning opportunities. 

% Another frontier in the application of \ac{snn}s in robotics is the integration of \ac{fl}. The \ac{fl} has been combined with \ac{snn}s to achieve collaborative learning across multiple agents while harnessing the efficiency of \ac{snn}s in distributed settings~\cite{Ch_1_R1}.

% In the exploration of integrating \ac{snn}s with \ac{fl}, several challenges emerge. The transmission of \ac{snn}-specific parameters, such as spike timings, introduces considerable communication overhead in the \ac{fl} setup~\cite{Int38}. Additionally, the aggregation of \ac{snn} models from diverse devices in a federated context is non-trivial, often leading to challenges in achieving effective global learning. The unique dynamics of spiking neurons, coupled with the distributed nature of \ac{fl}, can also result in training instabilities. Lastly, the inherent complexity and potential size of \ac{snn}s raise concerns about memory requirements and the feasibility of model aggregation in distributed scenarios~\cite{Ch_1_R3}.