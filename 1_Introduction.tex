\chapter{Introduction}

\section{Spiking Neural Networks: Models and Learning Algorithms}

\subsection{Neuron model}

In the study of computational neuroscience and the development of neural networks, particularly \ac{snn}, various neuron models have been proposed to simulate the electrical activity of neurons. These models range from simple to complex, aiming to capture the essential features of neuronal dynamics. This section provides an overview of several key neuron models, including their equations and characteristics.


\subsubsection{Leaky-Integrate and Fire (LIF) model}
The LIF model is a biological model that can be represented as a circuit with a resistor and capacitor and represents a first-order dynamic system \cite{Ch5_NM1},

\begin{equation} \label{Eq.1}
    R_{m}C_{m}\frac{dV_m\left(t\right)}{dt} = E_{l} - V_m\left(t\right) + R_{m}I\left(t\right)
\end{equation}

\noindent where $V_{m}(t)$ is the neuron's membrane potential, $R_{m}$ is the membrane resistance, $C_{m}$ is the membrane capacitance, $E_{l}$ is the resting potential, and $I(t)$ is the input current. The neuron spikes when its potential reaches the threshold potential ($V_{th}$). The potential of the neuron immediately reaches the reset potential ($V_{res}$) after it spikes.

The spike rate is a parameter that determines how fast the neuron spikes \cite{Ch5_NM2}.

\begin{equation} \label{Eq.4}
    r_{[Hz]} = \frac{1}{t_{isi}\enspace [s]}
\end{equation}

\noindent where $t_{isi}$ is the inter-spike interval that can be calculated using the neuron model. When the potential of a neuron reaches the threshold potential, it fires. Therefore, based on the analytical solution of (\ref{Eq.1}), the inter-spike interval time can be written as,

\begin{equation} \label{Eq.7}
    t_{isi} = \tau_{m}\ln\left(\frac{E_{l} + R_{m}I - V_{res}}{E_{l} + R_{m}I - V_{th}}\right)
\end{equation}

\noindent where $\tau_{m}$ is the membrane time constant. 

According to (\ref{Eq.7}), the following condition should be satisfied to have a finite value for $t_{isi}$,

\begin{equation} \label{Eq.8}
    E_{l} + R_{m}I - V_{th} > 0
\end{equation}

\noindent or

\begin{equation} \label{Eq.9}
    I > \frac{V_{th} - E_{l}}{R_{m}}
\end{equation}

\noindent which means that the input current higher than the above value generates spikes. 

After calculating the minimum input for neurons, we must find the maximum input based on the inter-spike interval. Equation (\ref{Eq.7}) can be written as,

\begin{equation} \label{Eq.10}
    t_{isi} = \tau_{m}\ln\left(1+\frac{V_{th} - V_{res}}{E_{l} + R_{m}I - V_{th}}\right)
\end{equation}

\noindent Equation (\ref{Eq.10}) can be approximated using the Maclaurin series for the natural logarithm function ($\ln(1+z)\approx z$) as follows,

\begin{equation} \label{Eq.11}
    t_{isi} = \frac{\tau_{m}\left(V_{th} - V_{res}\right)}{E_{l} + R_{m}I - V_{th}}
\end{equation}

\noindent Solving for $I$, an input current as a function of the inter-spike interval can be obtained,

\begin{equation} \label{Eq.12}
    I = \frac{\tau_{m}\left(V_{th} - V_{res}\right)}{t_{isi}R_{m}} + \frac{V_{th} - E_{l}}{R_{m}}
\end{equation}

The maximum value for the input current makes the neuron fire at each sample time ($\Delta t$). Therefore, the maximum input current is,

\begin{equation} \label{Eq.13}
    I^{max} = \frac{\tau_{m}\left(V_{th} - V_{res}\right)}{\Delta t R_{m}} + \frac{V_{th} - E_{l}}{R_{m}}
\end{equation}

In this section, we obtained the minimum and maximum values for input current using (\ref{Eq.9}) and (\ref{Eq.13}). These equations are used in the learning and encoding processes of the SNN.

\subsubsection{Hodgkin-Huxley Model}

The Hodgkin-Huxley model offers a detailed description of the ionic mechanisms underlying the initiation and propagation of action potentials in neurons:

\begin{align}
C_m \frac{dV}{dt} &= I_{ext} - \bar{g}_{K}n^4(V-V_{K}) - \bar{g}_{Na}m^3h(V-V_{Na}) - \bar{g}_{L}(V-V_{L}) \\
\frac{dn}{dt} &= \alpha_n(V)(1-n) - \beta_n(V)n \\
\frac{dm}{dt} &= \alpha_m(V)(1-m) - \beta_m(V)m \\
\frac{dh}{dt} &= \alpha_h(V)(1-h) - \beta_h(V)h
\end{align}

where $C_m$ is the membrane capacitance, $V$ is the membrane potential, $I_{ext}$ is the external current, $\bar{g}_i$ and $V_i$ are the maximum conductances and reversal potentials for the $K^+$, $Na^+$, and leak ($L$) currents, and $n$, $m$, and $h$ are gating variables. The variables \(n\), \(m\), and \(h\) are dimensionless and vary between 0 and 1, representing the proportion of ion channels in a particular state. The dynamics of these gating variables are critical for the model's ability to replicate the complex temporal patterns of neuronal action potentials.

\subsubsection{FitzHugh-Nagumo Model}

The FitzHugh-Nagumo model simplifies the Hodgkin-Huxley model into a two-variable system to capture the essential features of excitability:

\begin{align}
\frac{dV}{dt} &= V - \frac{V^3}{3} - W + I_{ext} \\
\frac{dW}{dt} &= 0.08(V + 0.7 - 0.8W)
\end{align}

where $V$ represents the membrane potential, $W$ is a recovery variable, and $I_{ext}$ is the external current.

\subsubsection{Izhikevich Model}

The Izhikevich model combines the biological plausibility of the Hodgkin-Huxley model with the computational efficiency of integrate-and-fire models:

\begin{align}
\frac{dv}{dt} &= 0.04v^2 + 5v + 140 - u + I \\
\frac{du}{dt} &= a(bv - u)
\end{align}

when $v \geq 30$ mV, then $v \leftarrow c$ and $u \leftarrow u + d$. Here, $v$ and $u$ represent the membrane potential and membrane recovery variable, respectively, and $a$, $b$, $c$, and $d$ are parameters of the model, with $I$ representing the current.


\subsection{Training Algorithms}

\subsubsection{Reward-modulated Spike-Timing-Dependent Plasticity (R-STDP)}

The \ac{stdp} algorithm is a learning technique inspired by biological processes in the brain, which is believed to be fundamental to certain learning processes \cite{STDP2}. The algorithm's core principle is that when a pre-synaptic neuron activates just before its post-synaptic counterpart, the synapse's strength connecting them should increase, and vice versa if the post-synaptic neuron fires first.

Within the \ac{snn} framework, pre-synaptic neurons are the input neurons, and post-synaptic neurons function as the output neurons. The function \(STDP(\tau)\) can be defined as the firing timelines of both input and output neurons \cite{STDP1} as,

\begin{equation} \label{Eq.TA.1}
STDP_{kl}(\tau) = \mathcal{A} \exp\left(-\frac{\tau}{\tau_{s}}\right) \text{for \(\tau \geq 0\)}
\end{equation}

\noindent where \(\mathcal{A}\) stands as the exponential function's amplitude, and \(\tau\) is the difference between the firing time of the input neuron \((k)\) and output neuron \((l)\). Meanwhile, \(\tau_{s}\) acts as the time constant, setting the decay rate for the \(\ac{stdp}\) function. Should \(\tau_{s}\) approach infinity, the exponential function converges to 1, neutralizing time's effect on the \(\ac{stdp}\) function.

The adjustment of synaptic weights follows the given equation:

\begin{equation} \label{Eq.TA.2}
\dot{W}_{kl}(t) = STDP_{kl}(\tau)\mathcal{R}(t)
\end{equation}

\noindent where \(\dot{W}_{kl}(t)\) denotes the rate of change of the synaptic weight that connects neurons \(k\) and \(l\). This weight determines the input that the post-synaptic neuron receives upon the spiking of its pre-synaptic neuron, which is quantified as \(I(t)\).

\subsubsection{Temporal Difference Learning (TD-Learning)}

TD-Learning for SNNs employs the concept of temporal difference errors to adjust synaptic weights, facilitating the learning of predictions about future rewards:

\begin{equation}
\Delta W_{kl} = \alpha \cdot (r + \gamma \cdot V(s') - V(s)) \cdot STDP_{kl}(\tau)
\end{equation}

Here, \(\alpha\) is the learning rate, \(r\) represents the immediate reward, and \(\gamma\) is the discount factor for future rewards. \(V(s)\) and \(V(s')\) denote the values of the current and subsequent states, respectively.

\subsubsection{Deep Q-Learning for Spiking Neural Networks (DQSNN)}

DQSNN integrates the principles of Deep Q-Learning with the dynamics of SNNs, allowing for the application of deep RL strategies:

\begin{equation}
\Delta W = \alpha \cdot (r + \gamma \cdot \max_{a'}Q(s',a';\theta') - Q(s,a;\theta)) \cdot \nabla_{\theta}Q(s,a;\theta)
\end{equation}

This equation updates synaptic weights \(\Delta W\) by optimizing the Q-value function \(Q(s,a;\theta)\) with respect to the network parameters \(\theta\), guided by the prediction error between expected and obtained rewards.

\subsubsection{Policy Gradient Methods for SNNs}

Policy Gradient Methods directly optimize the policy function \(\pi(a|s;\theta)\), encouraging actions that lead to higher rewards:

\begin{equation}
\Delta \theta = \alpha \cdot \mathbb{E}[\nabla_{\theta} \log \pi(a|s;\theta) \cdot R]
\end{equation}

where \(R\) is the reward associated with taking action \(a\) in state \(s\), and \(\mathbb{E}[]\) denotes the expectation over the distribution of actions.

\subsubsection{Spiking Actor-Critic (SAC)}

SAC employs an actor-critic architecture, with both components modeled as SNNs. The actor updates its policy based on the critic's value function estimates:

\begin{align}
\Delta \theta_{actor} &= \alpha \cdot \nabla_{\theta} \log \pi(a|s;\theta) \cdot A(s,a) \\
\Delta \theta_{critic} &= \beta \cdot (r + \gamma \cdot V(s') - V(s)) \cdot \nabla_{\theta}V(s)
\end{align}

where \(A(s,a)\) is the advantage function, indicating the benefit of taking action \(a\) in state \(s\), and \(\beta\) is the learning rate for the critic.

\subsubsection{Spiking Proximal Policy Optimization (SPPO)}

SPPO adapts the Proximal Policy Optimization algorithm for SNNs, optimizing a clipped surrogate objective to improve learning stability:

\begin{equation}
L(\theta) = \mathbb{E}\left[\min\left(r_t(\theta) \cdot A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot A_t\right)\right]
\end{equation}

where \(r_t(\theta)\) is the probability ratio of the current and old policies, and \(\epsilon\) is a hyperparameter defining the clipping range.

\subsection{Discussion}

The development of RL-based algorithms for SNNs represents a significant advancement in leveraging the computational properties of spiking neurons for learning complex behaviors. By incorporating reward signals, prediction errors, and policy optimization strategies, these algorithms enable SNNs to adaptively refine their synaptic connections, fostering the emergence of intelligent decision-making capabilities. The continuous evolution of these algorithms promises to further unlock the potential of SNNs in a wide array of applications, from autonomous systems to advanced cognitive modeling.

\section{Federated Learning for SNNs: Consensus Flying Scenario}

